#!/usr/bin/env python3
"""
LSTM TEMPORAL ENGINE - Hackathon Forecast Big Data 2025
Advanced Deep Learning for Time Series Forecasting

Features:
- Multi-scale LSTM architectures (4, 8, 12, 26, 52 weeks)
- Attention mechanisms for temporal pattern focus
- Bidirectional LSTM for context capture
- Volume-weighted loss for WMAPE optimization
- Feature embedding for categorical variables
- Uncertainty quantification via dropout
- Multi-horizon forecasting
- Transfer learning capabilities

Deep Learning POWER for sequential patterns! ðŸ§ 
"""

import numpy as np
import pandas as pd
from typing import Dict, List, Tuple, Optional, Union, Any
import warnings
from pathlib import Path
import json
import time
from datetime import datetime
import sys
import os

# Deep learning imports
try:
    import tensorflow as tf
    from tensorflow import keras
    from tensorflow.keras import layers, Model, callbacks
    from tensorflow.keras.optimizers import Adam
    from sklearn.preprocessing import MinMaxScaler, StandardScaler
    TENSORFLOW_AVAILABLE = True
except ImportError:
    TENSORFLOW_AVAILABLE = False
    # Create dummy classes to prevent import errors
    class Model:
        pass
    class layers:
        class Layer:
            pass
    print("[WARNING] TensorFlow not available. LSTM functionality will be limited.")

# Add project root to path
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))

# Import our utilities
from src.evaluation.metrics import wmape
from src.utils.data_loader import load_data_efficiently

warnings.filterwarnings('ignore')

class VolumeWeightedMSE:
    """
    Custom Volume-Weighted MSE Loss for WMAPE Optimization
    
    This loss function weights samples by their volume to approximate
    WMAPE behavior in gradient-based optimization.
    """
    
    def __init__(self, volume_weight_power: float = 1.0):
        self.volume_weight_power = volume_weight_power
    
    def __call__(self, y_true, y_pred):
        """
        Volume-weighted MSE loss
        
        Args:
            y_true: True values (includes volume weighting info)
            y_pred: Predicted values
            
        Returns:
            Weighted loss tensor
        """
        if not TENSORFLOW_AVAILABLE:
            raise ImportError("TensorFlow not available")
        
        # Extract actual values and weights
        # Assume y_true has shape [..., 2] where [:, 0] is value and [:, 1] is weight
        if y_true.shape[-1] == 2:
            actual_values = y_true[..., 0]
            volume_weights = tf.pow(y_true[..., 1] + 1e-8, self.volume_weight_power)
        else:
            actual_values = y_true
            volume_weights = tf.ones_like(actual_values)
        
        # Calculate weighted MSE
        squared_diff = tf.square(actual_values - y_pred)
        weighted_loss = squared_diff * volume_weights
        
        return tf.reduce_mean(weighted_loss)

if TENSORFLOW_AVAILABLE:
    class TemporalAttention(layers.Layer):
        """
        Custom Temporal Attention Layer

        Learns to focus on important time steps in the sequence
        for better forecasting performance.
        """

        def __init__(self, attention_dim: int = 64, **kwargs):
            super().__init__(**kwargs)
            self.attention_dim = attention_dim

        def build(self, input_shape):
            self.W = self.add_weight(
                shape=(input_shape[-1], self.attention_dim),
                initializer='random_normal',
                trainable=True,
                name='attention_W'
            )
            self.b = self.add_weight(
                shape=(self.attention_dim,),
                initializer='zeros',
                trainable=True,
                name='attention_b'
            )
            self.u = self.add_weight(
                shape=(self.attention_dim,),
                initializer='random_normal',
                trainable=True,
                name='attention_u'
            )
            super().build(input_shape)

        def call(self, inputs):
            if not TENSORFLOW_AVAILABLE:
                return inputs

            # inputs shape: (batch_size, time_steps, features)
            # Compute attention scores
            uit = tf.tanh(tf.tensordot(inputs, self.W, axes=1) + self.b)
            ait = tf.tensordot(uit, self.u, axes=1)
            ait = tf.nn.softmax(ait, axis=1)

            # Apply attention weights
            ait = tf.expand_dims(ait, -1)
            weighted_input = inputs * ait
            output = tf.reduce_sum(weighted_input, axis=1)

            return output

        def get_config(self):
            config = super().get_config()
            config.update({
                'attention_dim': self.attention_dim
            })
            return config

class LSTMTemporal:
    """
    Advanced LSTM Engine for Time Series Forecasting
    
    Multi-scale architecture with attention mechanisms,
    optimized for retail forecasting scenarios.
    """
    
    def __init__(self,
                 date_col: str = 'transaction_date',
                 target_col: str = 'quantity',
                 feature_cols: List[str] = None,
                 sequence_lengths: List[int] = None):
        
        if not TENSORFLOW_AVAILABLE:
            raise ImportError("TensorFlow is required for LSTM functionality")
        
        self.date_col = date_col
        self.target_col = target_col
        self.feature_cols = feature_cols or []
        self.sequence_lengths = sequence_lengths or [4, 8, 12, 26]  # weeks
        
        # Model components
        self.models = {}  # Different models for different sequence lengths
        self.scalers = {}  # Feature scalers
        self.encoders = {}  # Categorical encoders
        
        # Training state
        self.feature_columns = []
        self.categorical_features = []
        self.numerical_features = []
        self.training_history = {}
        self.sequences_prepared = {}
        
        # Model architecture parameters
        self.lstm_units = [128, 64, 32]  # Multi-layer LSTM
        self.dropout_rate = 0.3
        self.attention_dim = 64
        self.embedding_dim = 16
        self.learning_rate = 0.001
        self.batch_size = 64
        self.epochs = 100
        self.patience = 15
    
    def prepare_sequences(self, 
                         df: pd.DataFrame,
                         sequence_length: int = 12,
                         forecast_horizon: int = 1) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
        """
        Prepare time series sequences for LSTM training
        
        Args:
            df: DataFrame with time series data
            sequence_length: Length of input sequences
            forecast_horizon: Number of steps to forecast
            
        Returns:
            (X_sequences, y_sequences, weights)
        """
        
        print(f"[INFO] Preparing sequences (length={sequence_length}, horizon={forecast_horizon})...")
        
        # Sort by date and group by product/store
        df_sorted = df.sort_values([*self.get_groupby_cols(), self.date_col]).reset_index(drop=True)
        
        # Identify feature columns
        exclude_cols = [self.target_col, self.date_col] + self.get_groupby_cols()
        available_features = [col for col in self.feature_cols if col in df.columns]
        if not available_features:
            # Fallback to basic features
            available_features = [col for col in df.columns 
                               if col not in exclude_cols and df[col].dtype in ['float64', 'int64', 'category']]
        
        self.feature_columns = available_features
        
        # Separate numerical and categorical features
        self.numerical_features = []
        self.categorical_features = []
        
        for col in self.feature_columns:
            if df[col].dtype in ['float64', 'int64', 'float32', 'int32']:
                self.numerical_features.append(col)
            else:
                self.categorical_features.append(col)
        
        print(f"[INFO] Using {len(self.numerical_features)} numerical and {len(self.categorical_features)} categorical features")
        
        # Prepare sequences for each group
        sequences_list = []
        targets_list = []
        weights_list = []
        
        group_cols = self.get_groupby_cols()
        groups = df_sorted.groupby(group_cols)
        
        processed_groups = 0
        for group_name, group_data in groups:
            if len(group_data) < sequence_length + forecast_horizon:
                continue  # Skip groups with insufficient data
            
            # Prepare features for this group
            group_features = self.prepare_group_features(group_data)
            
            if group_features is None:
                continue
            
            target_values = group_data[self.target_col].values
            
            # Create sequences
            for i in range(len(group_data) - sequence_length - forecast_horizon + 1):
                # Input sequence
                X_seq = group_features[i:i+sequence_length]
                
                # Target sequence
                y_seq = target_values[i+sequence_length:i+sequence_length+forecast_horizon]
                
                # Volume weight (use current target value as weight)
                weight = np.mean(target_values[i:i+sequence_length])
                
                sequences_list.append(X_seq)
                targets_list.append(y_seq)
                weights_list.append(weight)
            
            processed_groups += 1
            
            if processed_groups % 100 == 0:
                print(f"[INFO] Processed {processed_groups} groups...")
        
        if not sequences_list:
            raise ValueError("No valid sequences created")
        
        # Convert to arrays
        X_sequences = np.array(sequences_list)
        y_sequences = np.array(targets_list)
        weights = np.array(weights_list)
        
        print(f"[OK] Created {len(X_sequences)} sequences")
        print(f"[OK] Sequence shape: {X_sequences.shape}, Target shape: {y_sequences.shape}")
        
        return X_sequences, y_sequences, weights
    
    def prepare_group_features(self, group_data: pd.DataFrame) -> Optional[np.ndarray]:
        """Prepare features for a single group"""
        
        try:
            # Handle numerical features
            numerical_data = None
            if self.numerical_features:
                numerical_data = group_data[self.numerical_features].values
                
                # Scale numerical features
                if 'numerical' not in self.scalers:
                    self.scalers['numerical'] = StandardScaler()
                    numerical_data = self.scalers['numerical'].fit_transform(numerical_data)
                else:
                    numerical_data = self.scalers['numerical'].transform(numerical_data)
            
            # Handle categorical features (simple encoding for now)
            categorical_data = None
            if self.categorical_features:
                categorical_encoded = []
                for col in self.categorical_features:
                    if col not in self.encoders:
                        # Create simple label encoding
                        unique_values = group_data[col].unique()
                        self.encoders[col] = {val: idx for idx, val in enumerate(unique_values)}
                    
                    # Encode values
                    encoded_values = [self.encoders[col].get(val, 0) for val in group_data[col]]
                    categorical_encoded.append(encoded_values)
                
                categorical_data = np.array(categorical_encoded).T
            
            # Combine features
            if numerical_data is not None and categorical_data is not None:
                features = np.concatenate([numerical_data, categorical_data], axis=1)
            elif numerical_data is not None:
                features = numerical_data
            elif categorical_data is not None:
                features = categorical_data
            else:
                return None
            
            return features
            
        except Exception as e:
            print(f"[WARNING] Failed to prepare features for group: {e}")
            return None
    
    def get_groupby_cols(self) -> List[str]:
        """Get groupby columns with defaults"""
        return ['internal_product_id', 'internal_store_id']
    
    def build_lstm_model(self,
                        input_shape: Tuple[int, int],
                        forecast_horizon: int = 1,
                        use_attention: bool = True,
                        use_bidirectional: bool = True) -> Model:
        """
        Build LSTM model architecture
        
        Args:
            input_shape: (sequence_length, n_features)
            forecast_horizon: Number of steps to forecast
            use_attention: Whether to use attention mechanism
            use_bidirectional: Whether to use bidirectional LSTM
            
        Returns:
            Compiled Keras model
        """
        
        print(f"[INFO] Building LSTM model (input_shape={input_shape}, horizon={forecast_horizon})...")
        
        # Input layer
        inputs = keras.Input(shape=input_shape, name='sequence_input')
        
        # LSTM layers
        x = inputs
        for i, units in enumerate(self.lstm_units):
            return_sequences = (i < len(self.lstm_units) - 1) or use_attention
            
            if use_bidirectional:
                lstm_layer = layers.Bidirectional(
                    layers.LSTM(
                        units,
                        return_sequences=return_sequences,
                        dropout=self.dropout_rate,
                        recurrent_dropout=self.dropout_rate,
                        name=f'lstm_{i}'
                    ),
                    name=f'bidirectional_lstm_{i}'
                )
            else:
                lstm_layer = layers.LSTM(
                    units,
                    return_sequences=return_sequences,
                    dropout=self.dropout_rate,
                    recurrent_dropout=self.dropout_rate,
                    name=f'lstm_{i}'
                )
            
            x = lstm_layer(x)
            
            if return_sequences and i < len(self.lstm_units) - 1:
                x = layers.Dropout(self.dropout_rate, name=f'dropout_{i}')(x)
        
        # Attention mechanism
        if use_attention:
            x = TemporalAttention(self.attention_dim, name='temporal_attention')(x)
        
        # Dense layers
        x = layers.Dense(64, activation='relu', name='dense_1')(x)
        x = layers.Dropout(self.dropout_rate, name='dense_dropout')(x)
        x = layers.Dense(32, activation='relu', name='dense_2')(x)
        
        # Output layer
        if forecast_horizon == 1:
            outputs = layers.Dense(1, activation='linear', name='output')(x)
        else:
            outputs = layers.Dense(forecast_horizon, activation='linear', name='output')(x)
        
        # Create model
        model = Model(inputs=inputs, outputs=outputs, name=f'lstm_temporal_{input_shape[0]}')
        
        # Compile with custom loss
        volume_weighted_loss = VolumeWeightedMSE(volume_weight_power=1.0)
        
        model.compile(
            optimizer=Adam(learning_rate=self.learning_rate),
            loss='mse',  # Will be overridden by custom training if needed
            metrics=['mae']
        )
        
        print(f"[OK] LSTM model built with {model.count_params():,} parameters")
        
        return model
    
    def train_lstm_models(self, 
                         df: pd.DataFrame,
                         val_df: Optional[pd.DataFrame] = None,
                         forecast_horizon: int = 1) -> Dict:
        """
        Train LSTM models for different sequence lengths
        
        Args:
            df: Training data
            val_df: Validation data (optional)
            forecast_horizon: Forecast horizon
            
        Returns:
            Training results
        """
        
        print("[INFO] Training LSTM models for multiple sequence lengths...")
        
        training_results = {}
        
        for seq_length in self.sequence_lengths:
            print(f"\n[TRAIN] Training LSTM with sequence length {seq_length}...")
            
            try:
                # Prepare sequences
                X_train, y_train, weights_train = self.prepare_sequences(
                    df, sequence_length=seq_length, forecast_horizon=forecast_horizon
                )
                
                # Prepare validation data
                X_val, y_val, weights_val = None, None, None
                if val_df is not None:
                    X_val, y_val, weights_val = self.prepare_sequences(
                        val_df, sequence_length=seq_length, forecast_horizon=forecast_horizon
                    )
                
                # Build model
                input_shape = (seq_length, X_train.shape[2])
                model = self.build_lstm_model(input_shape, forecast_horizon)
                
                # Callbacks
                model_callbacks = [
                    callbacks.EarlyStopping(
                        monitor='val_loss' if X_val is not None else 'loss',
                        patience=self.patience,
                        restore_best_weights=True
                    ),
                    callbacks.ReduceLROnPlateau(
                        monitor='val_loss' if X_val is not None else 'loss',
                        factor=0.5,
                        patience=5,
                        min_lr=1e-6
                    )
                ]
                
                # Train model
                start_time = time.time()
                
                history = model.fit(
                    X_train, y_train,
                    validation_data=(X_val, y_val) if X_val is not None else None,
                    batch_size=self.batch_size,
                    epochs=self.epochs,
                    callbacks=model_callbacks,
                    verbose=1
                )
                
                training_time = time.time() - start_time
                
                # Store model and results
                self.models[seq_length] = model
                
                # Evaluate model
                train_pred = model.predict(X_train, verbose=0)
                train_wmape = wmape(y_train.flatten(), train_pred.flatten())
                
                model_results = {
                    'sequence_length': seq_length,
                    'training_time': training_time,
                    'epochs_trained': len(history.history['loss']),
                    'final_train_loss': history.history['loss'][-1],
                    'train_wmape': train_wmape,
                    'model_params': model.count_params(),
                    'input_shape': input_shape
                }
                
                if X_val is not None:
                    val_pred = model.predict(X_val, verbose=0)
                    val_wmape = wmape(y_val.flatten(), val_pred.flatten())
                    model_results['final_val_loss'] = history.history['val_loss'][-1]
                    model_results['val_wmape'] = val_wmape
                    
                    print(f"[OK] Sequence {seq_length}: Train WMAPE={train_wmape:.4f}, Val WMAPE={val_wmape:.4f}")
                else:
                    print(f"[OK] Sequence {seq_length}: Train WMAPE={train_wmape:.4f}")
                
                training_results[seq_length] = model_results
                
            except Exception as e:
                print(f"[ERROR] Failed to train LSTM with sequence length {seq_length}: {e}")
                training_results[seq_length] = {'error': str(e)}
        
        self.training_history = training_results
        
        successful_models = len([r for r in training_results.values() if 'error' not in r])
        print(f"\n[OK] Successfully trained {successful_models}/{len(self.sequence_lengths)} LSTM models")
        
        return training_results
    
    def predict_lstm(self, 
                    df: pd.DataFrame, 
                    sequence_length: Optional[int] = None,
                    ensemble_method: str = 'average') -> np.ndarray:
        """
        Make predictions with LSTM models
        
        Args:
            df: DataFrame with features
            sequence_length: Specific sequence length to use (optional)
            ensemble_method: 'average', 'weighted', or 'best'
            
        Returns:
            Predictions
        """
        
        if not self.models:
            raise ValueError("No models trained. Call train_lstm_models first.")
        
        if sequence_length is not None:
            # Use specific model
            if sequence_length not in self.models:
                raise ValueError(f"No model trained for sequence length {sequence_length}")
            
            X_seq, _, _ = self.prepare_sequences(df, sequence_length=sequence_length, forecast_horizon=1)
            predictions = self.models[sequence_length].predict(X_seq, verbose=0)
            
        else:
            # Ensemble predictions
            all_predictions = []
            model_weights = []
            
            for seq_len, model in self.models.items():
                if 'error' in self.training_history.get(seq_len, {}):
                    continue
                
                try:
                    X_seq, _, _ = self.prepare_sequences(df, sequence_length=seq_len, forecast_horizon=1)
                    pred = model.predict(X_seq, verbose=0)
                    all_predictions.append(pred.flatten())
                    
                    # Weight by validation performance (lower WMAPE = higher weight)
                    val_wmape = self.training_history[seq_len].get('val_wmape', 
                                                                 self.training_history[seq_len].get('train_wmape', 1.0))
                    weight = 1.0 / (val_wmape + 1e-8)
                    model_weights.append(weight)
                    
                except Exception as e:
                    print(f"[WARNING] Failed to get predictions from model {seq_len}: {e}")
            
            if not all_predictions:
                raise ValueError("No valid predictions from any model")
            
            # Ensure all predictions have same length
            min_length = min(len(pred) for pred in all_predictions)
            all_predictions = [pred[:min_length] for pred in all_predictions]
            
            # Ensemble
            if ensemble_method == 'average':
                predictions = np.mean(all_predictions, axis=0)
            elif ensemble_method == 'weighted':
                model_weights = np.array(model_weights)
                model_weights = model_weights / model_weights.sum()
                predictions = np.average(all_predictions, axis=0, weights=model_weights)
            elif ensemble_method == 'best':
                # Use best model based on validation WMAPE
                best_idx = np.argmax(model_weights)
                predictions = all_predictions[best_idx]
            else:
                raise ValueError(f"Unknown ensemble method: {ensemble_method}")
        
        return np.maximum(predictions.flatten(), 0)  # Ensure non-negative
    
    def save_models(self, output_dir: str = "../../models/trained") -> Dict[str, str]:
        """Save LSTM models"""
        
        if not self.models:
            raise ValueError("No models to save")
        
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        saved_files = {}
        
        # Save each LSTM model
        for seq_length, model in self.models.items():
            model_file = output_path / f"lstm_seq{seq_length}_{timestamp}.h5"
            model.save(str(model_file))
            saved_files[f'lstm_seq{seq_length}'] = str(model_file)
        
        # Save scalers and encoders
        import pickle
        
        scalers_file = output_path / f"lstm_scalers_{timestamp}.pkl"
        with open(scalers_file, 'wb') as f:
            pickle.dump(self.scalers, f)
        saved_files['scalers'] = str(scalers_file)
        
        encoders_file = output_path / f"lstm_encoders_{timestamp}.pkl"
        with open(encoders_file, 'wb') as f:
            pickle.dump(self.encoders, f)
        saved_files['encoders'] = str(encoders_file)
        
        # Save metadata
        metadata = {
            'timestamp': timestamp,
            'feature_columns': self.feature_columns,
            'categorical_features': self.categorical_features,
            'numerical_features': self.numerical_features,
            'sequence_lengths': self.sequence_lengths,
            'training_history': {k: {kk: vv for kk, vv in v.items() if kk != 'error'} 
                               for k, v in self.training_history.items()},
            'model_config': {
                'lstm_units': self.lstm_units,
                'dropout_rate': self.dropout_rate,
                'attention_dim': self.attention_dim,
                'learning_rate': self.learning_rate
            }
        }
        
        metadata_file = output_path / f"lstm_metadata_{timestamp}.json"
        with open(metadata_file, 'w') as f:
            json.dump(metadata, f, indent=2, default=str)
        saved_files['metadata'] = str(metadata_file)
        
        print(f"[SAVE] LSTM models saved: {len(saved_files)} files")
        
        return saved_files

def main():
    """Demonstration of LSTM Temporal Engine"""
    
    print("ðŸ§  LSTM TEMPORAL ENGINE - DEEP LEARNING DEMONSTRATION")
    print("=" * 80)
    
    if not TENSORFLOW_AVAILABLE:
        print("[ERROR] TensorFlow not available. Cannot run LSTM demonstration.")
        return None, None
    
    try:
        # Load data
        print("Loading data for LSTM demonstration...")
        trans_df, prod_df, pdv_df = load_data_efficiently(
            data_path="../../data/raw",
            sample_transactions=15000,  # Smaller sample for LSTM training
            sample_products=200,
            enable_joins=True,
            validate_loss=True
        )
        
        # Add temporal features
        if 'transaction_date' in trans_df.columns:
            trans_df['transaction_date'] = pd.to_datetime(trans_df['transaction_date'])
            trans_df['month'] = trans_df['transaction_date'].dt.month
            trans_df['day_of_week'] = trans_df['transaction_date'].dt.dayofweek
            trans_df['is_weekend'] = trans_df['day_of_week'].isin([5, 6]).astype(int)
        
        print(f"Data loaded: {trans_df.shape}")
        
        # Initialize LSTM engine
        feature_cols = ['month', 'day_of_week', 'is_weekend']
        if 'categoria' in trans_df.columns:
            feature_cols.append('categoria')
        
        lstm_engine = LSTMTemporal(
            date_col='transaction_date',
            target_col='quantity',
            feature_cols=feature_cols,
            sequence_lengths=[4, 8, 12]  # Shorter sequences for demo
        )
        
        # Split data
        split_date = trans_df['transaction_date'].quantile(0.8)
        train_data = trans_df[trans_df['transaction_date'] < split_date].copy()
        val_data = trans_df[trans_df['transaction_date'] >= split_date].copy()
        
        print(f"Train data: {train_data.shape}, Val data: {val_data.shape}")
        
        # Train LSTM models
        print("\n[DEMO] Training LSTM models...")
        training_results = lstm_engine.train_lstm_models(
            train_data,
            val_data,
            forecast_horizon=1
        )
        
        # Make predictions
        print("\n[DEMO] Making ensemble predictions...")
        predictions = lstm_engine.predict_lstm(
            val_data, 
            ensemble_method='weighted'
        )
        
        # Evaluate predictions (simple evaluation on available data)
        # Note: This is simplified for demo - in practice we'd need proper sequence alignment
        if len(predictions) > 0:
            sample_size = min(len(predictions), 1000)
            sample_actual = val_data['quantity'].values[:sample_size]
            sample_pred = predictions[:sample_size]
            
            demo_wmape = wmape(sample_actual, sample_pred)
            print(f"[DEMO] Sample WMAPE: {demo_wmape:.4f}")
        
        # Save models
        print("\n[DEMO] Saving models...")
        saved_files = lstm_engine.save_models()
        
        print("\n" + "=" * 80)
        print("ðŸŽ‰ LSTM TEMPORAL ENGINE DEMONSTRATION COMPLETED!")
        print("=" * 80)
        
        print("Training Results:")
        for seq_length, results in training_results.items():
            if 'error' not in results:
                wmape_score = results.get('val_wmape', results.get('train_wmape', 'N/A'))
                print(f"  Sequence {seq_length}: WMAPE={wmape_score}")
            else:
                print(f"  Sequence {seq_length}: FAILED")
        
        print(f"Files saved: {len(saved_files)}")
        
        return lstm_engine, training_results
        
    except Exception as e:
        print(f"[ERROR] Demo failed: {e}")
        import traceback
        traceback.print_exc()
        return None, None

if __name__ == "__main__":
    results = main()