# üèÜ Estrat√©gia Definitiva para Vencer o Hackathon Forecast Big Data 2025

## üìã Vis√£o Geral Executiva

### Objetivo Central

Desenvolver uma solu√ß√£o de **forecast de vendas superiora ao baseline da Big Data** atrav√©s de uma combina√ß√£o estrat√©gica de:

- **Feature engineering sofisticado** baseado em domain expertise de varejo
- **Portfolio diversificado de modelos** com ensemble inteligente
- **Valida√ß√£o temporal robusta** para evitar overfitting
- **Execu√ß√£o t√©cnica impec√°vel** com c√≥digo reproduz√≠vel e documentado

### Contexto do Problema

- **Dom√≠nio**: Varejo - Reposi√ß√£o de produtos (One-Click Order)
- **Target**: Quantidade semanal de vendas por PDV/SKU
- **Per√≠odo**: 4 semanas de janeiro/2023
- **Dados**: 1 ano hist√≥rico (2022) + cadastros
- **M√©trica**: WMAPE (Weighted Mean Absolute Percentage Error)
- **Benchmark cr√≠tico**: Superar algoritmo interno da Big Data

### Fatores Cr√≠ticos de Sucesso

1. üéØ **Superar baseline da Big Data** - Requisito obrigat√≥rio
2. üíª **C√≥digo execut√°vel** - Condi√ß√£o eliminat√≥ria
3. üìä **Feature engineering superior** - Principal diferencial
4. ü§ñ **Ensemble robusto** - Redu√ß√£o de vari√¢ncia
5. üìù **Documenta√ß√£o exemplar** - Impressionar jurados

---

## üîç An√°lise de Baseline e Benchmarking

### Entendimento do Baseline da Big Data

- **Hip√≥tese**: Provavelmente usa modelos tradicionais (ARIMA, Exponential Smoothing, Linear Regression)
- **Limita√ß√µes esperadas**:
  - Features b√°sicas (lags simples, m√©dias m√≥veis)
  - Tratamento limitado de sazonalidade complexa
  - Sem ensemble sofisticado
  - Pouco tratamento de casos especiais (cold start, intermittency)

### Estrat√©gia de Supera√ß√£o

- **Feature engineering avan√ßado**: Features que modelos simples n√£o conseguem capturar
- **Ensemble diversificado**: Combina√ß√£o de m√∫ltiplas abordagens
- **Tratamento especializado**: Cold start, produtos intermitentes, sazonalidade complexa
- **Domain expertise**: Insights espec√≠ficos de varejo e reposi√ß√£o

### Como Medir Progresso

- **M√©tricas intermedi√°rias**: WMAPE por segmento (produto, PDV, categoria)
- **Valida√ß√£o temporal**: Performance consistente em m√∫ltiplos per√≠odos
- **An√°lise de erros**: Entender onde estamos superando m√©todos tradicionais

---

## üí° Context One-Click Order & Domain Expertise

### Entendimento do Neg√≥cio

- **One-Click Order**: Sistema automatizado de reposi√ß√£o de produtos
- **Objetivo**: Evitar rupturas de estoque mantendo n√≠veis √≥timos
- **Desafios t√≠picos**:
  - Produtos com demanda intermitente
  - Sazonalidade complexa (semanal, mensal, eventos)
  - Diferen√ßas regionais e por tipo de PDV
  - Produtos novos sem hist√≥rico

### Insights de Dom√≠nio para Features

- **Ciclos de reposi√ß√£o**: Frequ√™ncia t√≠pica de pedidos por produto
- **Complementaridade**: Produtos vendidos em conjunto
- **Substitui√ß√£o**: Produtos que competem entre si
- **Lifecycle**: Fase do produto (lan√ßamento, maduro, descontinuado)
- **Regional patterns**: Diferen√ßas por regi√£o/tipo de estabelecimento

---

## üõ†Ô∏è Estrat√©gia de Dados Mock para Desenvolvimento

### Necessidade de Dados Sint√©ticos

- **Problema**: Dados reais s√≥ dispon√≠veis no in√≠cio do hackathon
- **Solu√ß√£o**: Criar dados sint√©ticos realistas para desenvolvimento antecipado

### Estrutura de Dados Mock Detalhada

```python
# Transa√ß√µes sint√©ticas (realistas)
import numpy as np
import pandas as pd
from scipy import stats

# Par√¢metros realistas
n_pdvs = 300
n_produtos = 2000
n_weeks = 52

# Gera√ß√£o com padr√µes realistas
def create_realistic_sales():
    # Base seasonality patterns
    seasonal_weekly = np.sin(2 * np.pi * np.arange(52) / 52)
    seasonal_monthly = np.sin(2 * np.pi * np.arange(52) / 13)
    
    # Product categories with different behaviors
    categories = {
        'beverages': {'base_volume': 100, 'seasonality_strength': 1.2},
        'snacks': {'base_volume': 80, 'seasonality_strength': 0.8},
        'cigarettes': {'base_volume': 150, 'seasonality_strength': 0.3},
        'grocery': {'base_volume': 60, 'seasonality_strength': 1.5}
    }
    
    # PDV types with different characteristics
    pdv_types = {
        'c-store': {'multiplier': 1.0, 'volatility': 0.3},
        'g-store': {'multiplier': 1.8, 'volatility': 0.2},
        'liquor': {'multiplier': 0.7, 'volatility': 0.5}
    }
    
    return generate_synthetic_transactions()

# Cadastro de produtos sint√©tico (detalhado)
products_mock = {
    'produto': range(1, 2001),
    'categoria': np.random.choice(['beverages', 'snacks', 'cigarettes', 'grocery'], 2000),
    'subcategoria': generate_subcategories(),
    'marca': generate_brands(),
    'preco_medio': stats.lognorm.rvs(s=0.5, scale=10, size=2000),
    'lifecycle_stage': np.random.choice(['launch', 'growth', 'mature', 'decline'], 2000)
}

# Cadastro de PDVs sint√©tico (detalhado)
pdvs_mock = {
    'pdv': range(1001, 1301),
    'tipo': np.random.choice(['c-store', 'g-store', 'liquor'], 300, p=[0.6, 0.3, 0.1]),
    'on_off_prem': np.random.choice(['on', 'off'], 300, p=[0.7, 0.3]),
    'zipcode': generate_zipcodes(),
    'regiao': generate_regions(),
    'size_tier': np.random.choice(['small', 'medium', 'large'], 300, p=[0.5, 0.3, 0.2])
}
```

### Pipeline de Testes

- **Desenvolvimento**: Usar dados mock para criar pipeline completo
- **Valida√ß√£o**: Testar todos os componentes com dados sint√©ticos
- **Transi√ß√£o**: Switch r√°pido para dados reais quando dispon√≠veis

---

## üéØ Fase 1: Prepara√ß√£o e Setup Estrat√©gico (Expandido)

### 1.1 Entendimento Profundo do Problema

- **Analisar WMAPE**: Entender como a m√©trica penaliza diferentes tipos de erro
- **Estudar literatura**: Papers sobre retail forecasting e demand planning
- **Benchmarking**: Estudar solu√ß√µes existentes (Prophet, DeepAR, etc.)
- **Domain research**: Entender padr√µes t√≠picos de varejo

### 1.2 An√°lise de Recursos Computacionais (Expandida)

#### Avalia√ß√£o de Hardware
```python
# Resource Assessment
resource_levels = {
    'minimal': {
        'ram': '< 8GB',
        'cpu': '< 4 cores',
        'gpu': None,
        'strategy': 'lightweight_models_only',
        'models': ['LinearRegression', 'Prophet', 'Simple ARIMA'],
        'max_features': 50,
        'ensemble': False
    },
    'standard': {
        'ram': '8-16GB',
        'cpu': '4-8 cores', 
        'gpu': 'Optional',
        'strategy': 'balanced_approach',
        'models': ['LightGBM', 'XGBoost', 'Prophet', 'LSTM (small)'],
        'max_features': 200,
        'ensemble': 'simple_averaging'
    },
    'optimal': {
        'ram': '16GB+',
        'cpu': '8+ cores',
        'gpu': 'Available',
        'strategy': 'full_pipeline',
        'models': ['All models', 'Complex ensembles'],
        'max_features': 'unlimited',
        'ensemble': 'advanced_stacking'
    }
}
```

#### Cloud Strategy Detalhada
- **Free tiers**: Google Colab Pro, Kaggle Kernels, AWS Free Tier
- **Paid options**: AWS EC2, Google Cloud, Azure ML
- **Cost optimization**: Spot instances, preemptible VMs
- **Data storage**: S3, Google Cloud Storage para datasets grandes
- **Backup compute**: Local + cloud hybrid approach

### 1.3 Setup do Ambiente de Desenvolvimento (Detalhado)

```
projeto_hackathon/
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ raw/           # Dados originais
‚îÇ   ‚îú‚îÄ‚îÄ processed/     # Dados limpos
‚îÇ   ‚îú‚îÄ‚îÄ features/      # Features engineered
‚îÇ   ‚îî‚îÄ‚îÄ mock/          # Dados sint√©ticos para testes
‚îú‚îÄ‚îÄ notebooks/
‚îÇ   ‚îú‚îÄ‚îÄ 01_eda/        # An√°lise explorat√≥ria
‚îÇ   ‚îú‚îÄ‚îÄ 02_features/   # Feature engineering
‚îÇ   ‚îú‚îÄ‚îÄ 03_models/     # Experimentos de modelagem
‚îÇ   ‚îî‚îÄ‚îÄ 04_ensemble/   # Ensemble e valida√ß√£o
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ data/          # Data processing
‚îÇ   ‚îú‚îÄ‚îÄ features/      # Feature engineering
‚îÇ   ‚îú‚îÄ‚îÄ models/        # Model implementations
‚îÇ   ‚îî‚îÄ‚îÄ utils/         # Utilities
‚îú‚îÄ‚îÄ models/            # Saved models
‚îú‚îÄ‚îÄ submissions/       # Prediction files
‚îú‚îÄ‚îÄ docs/             # Documentation
‚îú‚îÄ‚îÄ tests/            # Unit tests
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ Makefile
‚îî‚îÄ‚îÄ README.md
```

### 1.4 Versionamento e Tracking

- **Git strategy**: Branches por experimento
- **MLflow setup**: Tracking de experimentos
- **Config management**: Hydra ou similar
- **Results logging**: Structured logging para todas as runs

---

## üîç Fase 2: An√°lise Explorat√≥ria Estrat√©gica (Expandido)

### 2.1 An√°lise Temporal Profunda

- **Padr√µes semanais**: Segunda vs fim de semana
- **Sazonalidade mensal**: In√≠cio vs fim de m√™s (pagamento)
- **Eventos especiais**: Feriados, promo√ß√µes, eventos regionais
- **Trends de longo prazo**: Crescimento/decl√≠nio por categoria
- **Autocorrela√ß√£o**: Identificar lags mais relevantes

### 2.2 Segmenta√ß√£o Estrat√©gica Avan√ßada

- **ABC Analysis**: Produtos por volume/faturamento
- **Cluster de comportamento**: Produtos com padr√µes similares
- **PDV profiling**: Segmenta√ß√£o por performance e caracter√≠sticas
- **Regional analysis**: Padr√µes geogr√°ficos (urbano vs rural)
- **Cross-category analysis**: Intera√ß√µes entre categorias

### 2.3 An√°lise de Qualidade e Consist√™ncia

- **Missing data patterns**: Sistem√°tico vs aleat√≥rio
- **Outlier detection**: M√©todos m√∫ltiplos (IQR, Z-score, Isolation Forest)
- **Data drift**: Mudan√ßas de distribui√ß√£o ao longo do tempo
- **Consistency checks**: Valida√ß√µes de integridade
- **Coverage analysis**: Completude por dimens√£o

### 2.4 Insights Espec√≠ficos de Varejo

- **Velocity analysis**: Fast vs slow-moving products
- **Seasonality detection**: M√©todos autom√°ticos (STL, X-13)
- **Cross-selling patterns**: Market basket analysis
- **Price elasticity**: Impacto de pre√ßos nas vendas
- **Promotion effects**: Detec√ß√£o e quantifica√ß√£o

---

## üõ†Ô∏è Fase 3: Feature Engineering Avan√ßado (Expandido)

### 3.1 Features Temporais Sofisticadas

```python
# Lags e Windows
- lags: [1, 2, 3, 4, 8, 12, 26, 52] semanas
- rolling_stats: mean, median, std, min, max, skew
- windows: [4, 8, 12, 26] semanas
- exponential_decay: alpha = [0.1, 0.3, 0.5]

# Sazonalidade Avan√ßada
- fourier_components: m√∫ltiplos per√≠odos (4, 13, 26, 52)
- day_of_week: dummy variables
- week_of_month: 1-4
- month_interactions: categoria √ó m√™s
- seasonal_strength: magnitude da sazonalidade

# Tend√™ncias e Derivadas
- linear_trends: slope em janelas m√≥veis
- acceleration: segunda derivada
- volatility: rolling std / rolling mean
- momentum: diferen√ßas percentuais

# Features Espec√≠ficas para WMAPE
- percentage_error_features: features que minimizam erro percentual
- volume_weighted_features: features ponderadas pelo volume
- relative_performance: performance vs m√©dia da categoria
- forecast_difficulty: indicador de dificuldade de previs√£o
```

### 3.2 Features de Agrega√ß√£o Inteligentes

```python
# Por Produto
- total_sales_product: soma hist√≥rica
- avg_sales_per_pdv: m√©dia por ponto
- product_volatility: CV das vendas
- product_growth_rate: tend√™ncia

# Por PDV
- portfolio_diversity: entropia das categorias
- pdv_total_volume: soma total
- pdv_growth_trajectory: tend√™ncia
- top_products_share: % dos top 10 produtos

# Cross-Features
- product_pdv_affinity: vendas produto no PDV / m√©dia
- category_region_share: share da categoria na regi√£o
- product_seasonality_match: alinhamento com sazonalidade
```

### 3.3 Features Comportamentais Avan√ßadas

```python
# Intermit√™ncia
- zero_weeks_ratio: % semanas com venda zero
- consecutive_zeros: m√°ximo de zeros consecutivos
- purchase_frequency: frequ√™ncia m√©dia de compra
- reorder_cycles: ciclos t√≠picos de recompra

# Lifecycle
- weeks_since_first_sale: idade do produto
- growth_stage: classifica√ß√£o autom√°tica
- maturity_indicator: estabilidade das vendas
- decline_indicator: detec√ß√£o de decl√≠nio

# Market Dynamics
- market_share_product: % do produto na categoria
- competitive_pressure: n√∫mero de substitutos
- cross_selling_strength: for√ßa das associa√ß√µes
- cannibalization_risk: competi√ß√£o interna
```

### 3.4 Features de Contexto Externo

```python
# Geogr√°ficos
- zipcode_density: PDVs por regi√£o
- urban_rural_indicator: classifica√ß√£o da √°rea
- economic_index: se dispon√≠vel, √≠ndices socioecon√¥micos
- distance_to_competitors: proximidade de concorrentes

# Temporais Externos
- holiday_proximity: dist√¢ncia de feriados
- payroll_calendar: proximity to typical payday
- school_calendar: in√≠cio/fim de per√≠odo letivo
- weather_seasonality: se aplic√°vel
```

---

## ü§ñ Fase 4: Estrat√©gia de Modelagem Avan√ßada (Expandido)

### 4.1 Portfolio de Modelos Diversificado

#### Time Series Models

```python
# Prophet
- seasonalities: weekly, monthly, yearly
- holidays: custom holiday calendar
- changepoint_detection: automatic trend changes
- mcmc_samples: uncertainty quantification

# ARIMA/SARIMA
- automated selection: auto_arima
- seasonal_decompose: STL decomposition
- residual_analysis: diagnostic plots
- confidence_intervals: prediction uncertainty

# State Space Models
- exponential_smoothing: Holt-Winters
- ets_models: automated ETS selection
- structural_time_series: trend + seasonal components
```

#### Tree-Based Models

```python
# LightGBM
- categorical_feature: automatic handling
- early_stopping: overfitting prevention
- feature_importance: SHAP values
- hyperparameter_tuning: Optuna

# XGBoost
- objective: reg:squarederror
- eval_metric: wmape (custom)
- tree_method: gpu_hist if available
- regularization: alpha, lambda tuning

# CatBoost
- cat_features: automatic detection
- overfitting_detector: built-in
- feature_interactions: automatic
- uncertainty: virtual_ensembles
```

#### Neural Networks

```python
# LSTM/GRU
- sequence_length: [12, 26, 52] weeks
- attention_mechanism: temporal attention
- bidirectional: capture future context
- dropout: regularization

# Transformer
- positional_encoding: temporal positions
- multi_head_attention: multiple patterns
- encoder_decoder: seq2seq architecture
- pre_training: if applicable

# Neural Prophet
- trend_components: automatic detection
- seasonality_components: learnable
- event_modeling: promotional effects
- uncertainty_quantification: built-in
```

### 4.2 Valida√ß√£o Temporal Robusta (Detalhada)

#### Time Series Cross-Validation

```python
# Walk-Forward Validation
- initial_train: 40 weeks
- forecast_horizon: 4 weeks
- step_size: 1 week
- n_splits: 8-10 splits

# Blocked Cross-Validation
- block_size: 4 weeks
- gap_size: 2 weeks (simulate data delay)
- purge_size: 1 week (avoid leakage)
- embargo: 1 week (trading-like validation)
```

#### Metrics Tracking

```python
# Primary Metric
- wmape: weighted mean absolute percentage error
- wmape_by_segment: por produto, PDV, categoria
- consistency: performance across time periods

# Secondary Metrics
- mae: mean absolute error
- rmse: root mean squared error
- mape: mean absolute percentage error
- directional_accuracy: trend prediction
```

### 4.3 Tratamento de Casos Especiais (Detalhado)

#### Cold Start Problem

```python
# New Products
- similarity_based: find similar products
- category_averages: use category patterns
- hierarchical_forecasting: top-down approach
- collaborative_filtering: user-item matrix

# New PDVs
- demographic_matching: similar PDV profiles
- geographic_clustering: regional patterns
- size_based_scaling: volume adjustments
```

#### Intermittent Demand

```python
# Zero-Inflated Models (Expandido)
- hurdle_models: binary + continuous
  * Stage 1: Predict probability of non-zero sales
  * Stage 2: Predict quantity given non-zero
  * Use different features for each stage
  
- zero_inflated_regression: two-part model
  * Logistic regression for zero/non-zero
  * Count regression for positive values
  * Handle overdispersion with negative binomial
  
- croston_method: specialized for intermittent
  * Simple exponential smoothing on non-zero values
  * Separate smoothing for inter-arrival times
  * SBA (Syntetos-Boylan) adjustment for bias
  
- bootstrap_sampling: uncertainty from zeros
  * Resample historical zero patterns
  * Generate prediction intervals
  * Account for zero inflation in metrics
  
# Advanced Intermittency Features
- intermittency_ratio: zeros / total_observations
- average_inter_demand_interval: avg weeks between sales
- demand_intensity: avg demand when > 0
- zero_streak_analysis: consecutive zero patterns
- reactivation_probability: likelihood of returning to positive sales
```

#### Seasonal Products

```python
# Strong Seasonality
- seasonal_decomposition: trend + seasonal + residual
- harmonic_regression: fourier series
- seasonal_naive: simple seasonal patterns
- regime_switching: different models per season
```

---

## üî¨ Fase 5: Otimiza√ß√£o e Refinamento (Expandido)

### 5.1 An√°lise de Erros Detalhada

#### Error Decomposition

```python
# By Dimensions
- error_by_product: identify problematic products
- error_by_pdv: problematic locations
- error_by_time: temporal patterns in errors
- error_by_volume: small vs large sales

# Error Patterns
- systematic_bias: consistent over/under prediction
- heteroskedasticity: variance changes
- autocorrelation: error correlation over time
- seasonal_errors: seasonal bias patterns
```

#### Residual Analysis

```python
# Statistical Tests
- ljung_box: autocorrelation in residuals
- jarque_bera: normality of residuals
- arch_test: heteroskedasticity
- runs_test: randomness

# Visual Diagnostics
- qq_plots: normality assessment
- residual_vs_fitted: homoskedasticity
- acf_pacf_plots: autocorrelation
- seasonal_plots: seasonal residuals
```

### 5.2 Model Calibration (Detalhado)

#### Probability Calibration

```python
# Calibration Methods
- platt_scaling: logistic regression
- isotonic_regression: monotonic mapping
- temperature_scaling: neural networks
- conformal_prediction: distribution-free

# Uncertainty Quantification
- prediction_intervals: confidence bounds
- quantile_regression: percentile predictions
- bootstrap_sampling: empirical distribution
- bayesian_methods: posterior sampling
```

### 5.3 Ensemble Strategy (Avan√ßado)

#### Stacking Architecture

```python
# Level 1 Models (Base Learners)
- prophet: seasonal patterns
- lightgbm: feature interactions
- lstm: temporal dependencies
- arima: time series structure

# Level 2 Model (Meta-Learner)
- linear_regression: simple combination
- ridge_regression: regularized combination
- neural_network: non-linear combination
- feature_based: conditional weighting
```

#### Dynamic Weighting

```python
# Conditional Ensemble
- product_based_weights: different weights per product
- time_based_weights: seasonal weight adjustments
- performance_based_weights: based on recent accuracy
- uncertainty_based_weights: confidence-weighted averaging
```

### 5.4 Post-Processing (Business Rules)

#### Business Constraints

```python
# Inventory Constraints
- minimum_order_quantity: MOQ constraints
- maximum_capacity: storage limitations
- integer_constraints: whole units only
- non_negativity: no negative predictions

# Business Logic
- demand_smoothing: avoid extreme fluctuations
- promotional_adjustments: known promotions
- lifecycle_adjustments: product phase considerations
- competitive_adjustments: market share constraints
```

---

## üíª Fase 6: Qualidade T√©cnica e Execu√ß√£o (Expandido)

### 6.1 C√≥digo de Produ√ß√£o (Detalhado)

#### Architecture Patterns

```python
# Design Patterns
- factory_pattern: model creation
- strategy_pattern: different algorithms
- observer_pattern: logging and monitoring
- pipeline_pattern: data flow

# Code Structure
src/
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ loaders.py      # Data loading utilities
‚îÇ   ‚îú‚îÄ‚îÄ preprocessors.py # Data cleaning
‚îÇ   ‚îî‚îÄ‚îÄ validators.py   # Data validation
‚îú‚îÄ‚îÄ features/
‚îÇ   ‚îú‚îÄ‚îÄ temporal.py     # Time-based features
‚îÇ   ‚îú‚îÄ‚îÄ aggregates.py   # Aggregation features
‚îÇ   ‚îî‚îÄ‚îÄ categorical.py  # Category features
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îú‚îÄ‚îÄ base.py        # Abstract base class
‚îÇ   ‚îú‚îÄ‚îÄ timeseries.py  # Time series models
‚îÇ   ‚îú‚îÄ‚îÄ ml_models.py   # ML models
‚îÇ   ‚îî‚îÄ‚îÄ ensemble.py    # Ensemble methods
‚îú‚îÄ‚îÄ evaluation/
‚îÇ   ‚îú‚îÄ‚îÄ metrics.py     # Custom metrics
‚îÇ   ‚îú‚îÄ‚îÄ validation.py  # Cross-validation
‚îÇ   ‚îî‚îÄ‚îÄ analysis.py    # Error analysis
‚îî‚îÄ‚îÄ utils/
    ‚îú‚îÄ‚îÄ config.py      # Configuration management
    ‚îú‚îÄ‚îÄ logging.py     # Logging utilities
    ‚îî‚îÄ‚îÄ helpers.py     # Helper functions
```

#### Configuration Management

```yaml
# config.yaml
data:
  path: "data/raw/"
  validation_split: 0.2

features:
  lag_features: [1, 2, 3, 4, 8, 12]
  rolling_windows: [4, 8, 12, 26]
  seasonal_periods: [4, 13, 26, 52]

models:
  lightgbm:
    num_leaves: 31
    learning_rate: 0.1
    objective: "regression"

  prophet:
    yearly_seasonality: true
    weekly_seasonality: true
    daily_seasonality: false

ensemble:
  method: "stacking"
  cv_folds: 5
  meta_model: "ridge"

evaluation:
  primary_metric: "wmape"
  cv_method: "time_series_split"
  n_splits: 8
```

### 6.2 Pipeline Automatizado (Completo)

#### MLOps Pipeline

```python
# Makefile
.PHONY: data features models ensemble evaluate submit

data:
    python src/data/load_data.py
    python src/data/clean_data.py
    python src/data/validate_data.py

features:
    python src/features/create_temporal_features.py
    python src/features/create_aggregate_features.py
    python src/features/create_categorical_features.py

models:
    python src/models/train_prophet.py
    python src/models/train_lightgbm.py
    python src/models/train_lstm.py

ensemble:
    python src/models/train_ensemble.py
    python src/evaluation/validate_ensemble.py

evaluate:
    python src/evaluation/comprehensive_evaluation.py
    python src/evaluation/generate_report.py

submit:
    python src/models/generate_predictions.py
    python src/utils/format_submission.py
```

### 6.3 Testing Strategy

```python
# tests/
‚îú‚îÄ‚îÄ test_data/
‚îÇ   ‚îú‚îÄ‚îÄ test_loaders.py
‚îÇ   ‚îú‚îÄ‚îÄ test_preprocessors.py
‚îÇ   ‚îî‚îÄ‚îÄ test_validators.py
‚îú‚îÄ‚îÄ test_features/
‚îÇ   ‚îú‚îÄ‚îÄ test_temporal.py
‚îÇ   ‚îú‚îÄ‚îÄ test_aggregates.py
‚îÇ   ‚îî‚îÄ‚îÄ test_categorical.py
‚îú‚îÄ‚îÄ test_models/
‚îÇ   ‚îú‚îÄ‚îÄ test_base.py
‚îÇ   ‚îú‚îÄ‚îÄ test_timeseries.py
‚îÇ   ‚îî‚îÄ‚îÄ test_ensemble.py
‚îî‚îÄ‚îÄ test_utils/
    ‚îú‚îÄ‚îÄ test_metrics.py
    ‚îî‚îÄ‚îÄ test_helpers.py

# pytest configuration
pytest.ini:
[tool:pytest]
testpaths = tests
python_files = test_*.py
python_classes = Test*
python_functions = test_*
addopts = -v --tb=short --strict-markers
```

---

## üìä M√©tricas de Valida√ß√£o e KPIs Intermedi√°rios

### 6.4 Monitoring Dashboard

```python
# MLflow Tracking
import mlflow

# Track experiments
with mlflow.start_run():
    mlflow.log_param("model", "ensemble")
    mlflow.log_metric("wmape_cv", cv_wmape)
    mlflow.log_metric("wmape_by_product", product_wmape)
    mlflow.log_artifact("feature_importance.png")
    mlflow.sklearn.log_model(model, "model")

# Validation Metrics
validation_metrics = {
    'primary': {
        'wmape_overall': float,
        'wmape_by_segment': dict,
        'consistency_score': float
    },
    'secondary': {
        'mae': float,
        'rmse': float,
        'directional_accuracy': float
    },
    'business': {
        'coverage_95': float,  # % predictions within 95% CI
        'bias_by_volume': dict,  # bias for different volume tiers
        'seasonal_accuracy': dict  # accuracy by season
    }
}
```

---

## üéØ Fase 7: Estrat√©gia de Submiss√µes (Refinada)

### 7.1 Submiss√£o Strategy (Detalhada)

#### Timeline Strategy

```python
# 5 Submissions Plan
submissions = {
    1: {
        'timing': 'Day 3',
        'model': 'Simple baseline (Prophet)',
        'purpose': 'Test pipeline and get initial score',
        'risk': 'Low',
        'expected_wmape': 'High (baseline)'
    },
    2: {
        'timing': 'Day 7',
        'model': 'Best single model (LightGBM)',
        'purpose': 'Validate feature engineering',
        'risk': 'Medium',
        'expected_wmape': 'Medium'
    },
    3: {
        'timing': 'Day 10',
        'model': 'Initial ensemble',
        'purpose': 'Test ensemble approach',
        'risk': 'Medium',
        'expected_wmape': 'Lower'
    },
    4: {
        'timing': 'Day 13',
        'model': 'Optimized ensemble',
        'purpose': 'Leaderboard-informed optimization',
        'risk': 'Medium-High',
        'expected_wmape': 'Low'
    },
    5: {
        'timing': 'Final day',
        'model': 'Final ensemble + post-processing',
        'purpose': 'Last optimization push',
        'risk': 'High',
        'expected_wmape': 'Lowest'
    }
}
```

### 7.2 Leaderboard Analysis Strategy

```python
# Competitive Intelligence
def analyze_competition():
    # Score gaps analysis
    score_gaps = calculate_gaps_to_top_positions()

    # Improvement needed
    improvement_needed = {
        'top_3': score_gaps['top_3'] * 1.05,  # 5% buffer
        'top_10': score_gaps['top_10'] * 1.05,
        'beat_baseline': baseline_score * 0.95  # 5% better than baseline
    }

    # Strategic decisions
    if improvement_needed['top_3'] < 2.0:  # if close to top 3
        strategy = 'aggressive_optimization'
    elif improvement_needed['beat_baseline'] > 10.0:  # far from baseline
        strategy = 'fundamental_improvements'
    else:
        strategy = 'incremental_optimization'

    return strategy
```

### 7.3 Risk Management

```python
# Submission Risk Assessment
def assess_submission_risk(model, validation_score):
    risk_factors = {
        'overfitting_risk': calculate_train_val_gap(model),
        'complexity_risk': assess_model_complexity(model),
        'data_leakage_risk': check_for_leakage(model),
        'execution_risk': test_prediction_pipeline(model)
    }

    overall_risk = weighted_average(risk_factors)

    if overall_risk > 0.7:
        return 'HIGH_RISK - Consider simpler model'
    elif overall_risk > 0.4:
        return 'MEDIUM_RISK - Additional validation needed'
    else:
        return 'LOW_RISK - Safe to submit'
```

---

## ‚ö†Ô∏è Contingency Planning

### 8.1 Scenario Planning

#### Scenario A: Hardware Limitations

```python
# Lightweight Model Strategy
contingency_models = {
    'low_memory': {
        'model': 'LinearRegression + feature selection',
        'features': 'top_50_features',
        'training_time': '< 30 minutes'
    },
    'medium_compute': {
        'model': 'LightGBM with reduced features',
        'features': 'top_200_features',
        'training_time': '< 2 hours'
    },
    'full_compute': {
        'model': 'Full ensemble',
        'features': 'all_features',
        'training_time': '< 12 hours'
    }
}
```

#### Scenario B: Data Quality Issues

```python
# Data Quality Contingency
data_issues_response = {
    'missing_data': {
        'strategy': 'robust_imputation',
        'methods': ['median', 'mode', 'interpolation'],
        'validation': 'impact_analysis'
    },
    'outliers': {
        'strategy': 'robust_models',
        'methods': ['clip', 'winsorize', 'robust_scaling'],
        'models': 'tree_based_preferred'
    },
    'data_drift': {
        'strategy': 'temporal_validation',
        'methods': ['adaptive_models', 'recent_data_weighting'],
        'monitoring': 'distribution_tracking'
    }
}
```

#### Scenario C: Model Performance Issues

```python
# Performance Contingency
if model_performance < baseline_threshold:
    # Diagnostic checklist
    diagnostics = {
        'feature_importance': 'check_feature_relevance',
        'overfitting': 'check_train_val_gap',
        'data_leakage': 'check_temporal_integrity',
        'model_selection': 'try_simpler_models'
    }

    # Recovery actions
    recovery_actions = {
        'feature_engineering': 'simplify_features',
        'model_complexity': 'reduce_complexity',
        'validation': 'more_robust_cv',
        'ensemble': 'fewer_base_models'
    }
```

---

## üé® Interpretabilidade e Explicabilidade

### 9.1 Model Interpretability Strategy

```python
# Interpretability Tools
interpretability_suite = {
    'global': {
        'feature_importance': 'SHAP, permutation importance',
        'partial_dependence': 'PDPs for key features',
        'interaction_effects': 'H-statistics, SHAP interactions'
    },
    'local': {
        'instance_explanation': 'LIME, SHAP values',
        'counterfactual': 'what-if scenarios',
        'prototype_examples': 'representative instances'
    },
    'temporal': {
        'time_series_decomposition': 'trend + seasonal + residual',
        'attribution_over_time': 'SHAP temporal plots',
        'regime_detection': 'changepoint analysis'
    }
}
```

### 9.2 Business Insights Generation

```python
# Actionable Insights
business_insights = {
    'product_insights': {
        'top_drivers': 'most important features per product',
        'seasonal_patterns': 'seasonal behavior by product',
        'cross_selling': 'product association rules',
        'lifecycle_stage': 'product maturity analysis'
    },
    'pdv_insights': {
        'performance_drivers': 'what makes PDVs successful',
        'geographic_patterns': 'regional differences',
        'portfolio_optimization': 'optimal product mix',
        'capacity_utilization': 'volume vs performance'
    },
    'temporal_insights': {
        'peak_seasons': 'high demand periods',
        'trend_analysis': 'growth/decline patterns',
        'anomaly_detection': 'unusual events',
        'forecasting_accuracy': 'prediction confidence by time'
    }
}
```

---

## üîÑ An√°lise Competitiva e Diferencia√ß√£o

### 10.1 Competitive Analysis

```python
# Differentiation Strategy
differentiation_factors = {
    'technical': {
        'advanced_features': 'novel feature engineering',
        'ensemble_sophistication': 'multi-level stacking',
        'uncertainty_quantification': 'confidence intervals',
        'robustness': 'multiple validation schemes'
    },
    'business': {
        'domain_expertise': 'retail-specific insights',
        'interpretability': 'actionable business insights',
        'scalability': 'production-ready code',
        'documentation': 'comprehensive explanation'
    },
    'innovation': {
        'novel_approaches': 'unique techniques',
        'creative_features': 'domain-inspired features',
        'ensemble_creativity': 'innovative combinations',
        'post_processing': 'business-aware adjustments'
    }
}
```

### 10.2 Presentation Strategy

```python
# Judging Criteria Alignment
presentation_strategy = {
    'technical_excellence': {
        'code_quality': 'clean, documented, testable',
        'methodology': 'rigorous, well-validated',
        'innovation': 'creative but sound approaches',
        'reproducibility': 'fully executable pipeline'
    },
    'business_impact': {
        'practical_value': 'real-world applicability',
        'insights': 'actionable recommendations',
        'scalability': 'production considerations',
        'roi_potential': 'business value demonstration'
    },
    'communication': {
        'clarity': 'clear explanation of approach',
        'storytelling': 'coherent narrative',
        'visualization': 'effective charts and plots',
        'executive_summary': 'concise key points'
    }
}
```

---

## ‚ö° Cronograma Executivo Detalhado

### Phase-by-Phase Timeline (14 dias)

#### Week 1: Foundation and Exploration

```python
# Days 1-2: Setup and EDA
day_1 = {
    'morning': 'Environment setup, data loading, initial exploration',
    'afternoon': 'Data quality assessment, missing value analysis',
    'evening': 'Basic visualizations, initial insights'
}

day_2 = {
    'morning': 'Temporal analysis, seasonality detection',
    'afternoon': 'Segmentation analysis, clustering',
    'evening': 'Feature ideas brainstorming, documentation'
}

# Days 3-4: Feature Engineering
day_3 = {
    'morning': 'Temporal features creation',
    'afternoon': 'Aggregate features, cross-features',
    'evening': 'Feature validation, first baseline model',
    'milestone': 'SUBMISSION 1 - Simple baseline'
}

day_4 = {
    'morning': 'Advanced features, behavioral features',
    'afternoon': 'Feature selection, importance analysis',
    'evening': 'Feature pipeline optimization'
}

# Days 5-7: Core Modeling
day_5 = {
    'morning': 'Prophet model development',
    'afternoon': 'LightGBM model development',
    'evening': 'LSTM model experimentation',
    'milestone': 'First competitive model trained'
}

day_6 = {
    'morning': 'Model validation, hyperparameter tuning',
    'afternoon': 'Cross-validation setup, performance analysis',
    'evening': 'Model comparison, best single model selection'
}

day_7 = {
    'morning': 'Model refinement, feature importance analysis',
    'afternoon': 'Error analysis, model diagnostics',
    'evening': 'Second submission preparation',
    'milestone': 'SUBMISSION 2 - Best single model'
}
```

#### Week 2: Optimization and Finalization

```python
# Days 8-10: Ensemble and Optimization
day_8 = {
    'morning': 'Ensemble architecture design',
    'afternoon': 'Stacking implementation, meta-model training',
    'evening': 'Ensemble validation, performance analysis',
    'milestone': 'Ensemble model working'
}

day_9 = {
    'morning': 'Ensemble optimization, weight tuning',
    'afternoon': 'Post-processing implementation',
    'evening': 'Business rules integration, constraint handling'
}

day_10 = {
    'morning': 'Final ensemble validation',
    'afternoon': 'Submission preparation, error analysis',
    'evening': 'Third submission, leaderboard analysis',
    'milestone': 'SUBMISSION 3 - Initial ensemble'
}

# Days 11-12: Quality and Polish
day_11 = {
    'morning': 'Code refactoring, documentation',
    'afternoon': 'Testing, pipeline validation',
    'evening': 'Repository organization, README creation',
    'milestone': 'Code ready for evaluation'
}

day_12 = {
    'morning': 'Interpretability analysis, insight generation',
    'afternoon': 'Presentation notebook creation',
    'evening': 'Executive summary writing'
}

# Days 13-14: Final Push
day_13 = {
    'morning': 'Leaderboard analysis, competitive intelligence',
    'afternoon': 'Final optimization based on standings',
    'evening': 'Fourth submission, risk assessment',
    'milestone': 'SUBMISSION 4 - Optimized model'
}

day_14 = {
    'morning': 'Last-minute improvements, final validation',
    'afternoon': 'Final submission preparation',
    'evening': 'SUBMISSION 5 - Final push',
    'milestone': 'Competition complete'
}
```

### Daily Checkpoints

```python
daily_checklist = {
    'progress_review': 'What was accomplished today?',
    'blocker_identification': 'What obstacles were encountered?',
    'next_day_planning': 'What are tomorrow\'s priorities?',
    'risk_assessment': 'Any new risks identified?',
    'backup_strategy': 'Contingency plans updated?'
}
```

---

## üéñÔ∏è Fatores Cr√≠ticos de Sucesso

### Technical Excellence

1. **Feature Engineering Superior**: O principal diferencial competitivo
2. **Robust Validation**: Evitar overfitting atrav√©s de valida√ß√£o temporal rigorosa
3. **Ensemble Mastery**: Combinar modelos de forma inteligente para reduzir vari√¢ncia
4. **Edge Case Handling**: Tratamento especial para cold start e intermittent demand
5. **Code Quality**: Execut√°vel, reproduz√≠vel, bem documentado

### Strategic Execution

1. **Domain Expertise**: Demonstrar conhecimento profundo do varejo
2. **Competitive Intelligence**: Monitorar e reagir ao leaderboard estrategicamente
3. **Risk Management**: Balancear inova√ß√£o com execu√ß√£o segura
4. **Resource Optimization**: Maximizar efici√™ncia computacional e temporal
5. **Presentation Excellence**: Impressionar jurados com qualidade t√©cnica

### Innovation Balance

1. **Creative Features**: Features √∫nicos baseados em domain expertise
2. **Proven Techniques**: Combinar inova√ß√£o com m√©todos comprovados
3. **Explainable AI**: Modelos complexos mas interpret√°veis
4. **Business Value**: Focar em aplicabilidade real, n√£o apenas accuracy
5. **Scalable Solution**: Considerar deployment em produ√ß√£o

---

## üö® Armadilhas Cr√≠ticas a Evitar

### Technical Pitfalls

1. **Data Leakage**: Nunca usar informa√ß√£o futura - validar rigorosamente
2. **Overfitting Temporal**: Sempre usar time-based splits, nunca shuffle
3. **Feature Leakage**: Cuidado com features que n√£o estar√£o dispon√≠veis na predi√ß√£o
4. **Validation Mismatch**: Garantir que CV simula exatamente o cen√°rio real
5. **Scale Issues**: Verificar se features est√£o na escala correta

### Strategic Mistakes

1. **Complexity Trap**: Modelo simples funcionando > modelo complexo quebrado
2. **Deadline Rush**: Deixar tempo adequado para polimento e testes
3. **Single Point of Failure**: Ter sempre planos B funcionando
4. **Documentation Neglect**: Documenta√ß√£o pobre pode custar pontos
5. **Submission Panic**: Usar as 5 tentativas estrategicamente, n√£o desesperadamente

### Business Oversights

1. **Baseline Ignorance**: Focar obsessivamente em superar o benchmark da Big Data
2. **Domain Blindness**: Ignorar conhecimento de varejo espec√≠fico
3. **Interpretability Gap**: Modelo black-box pode perder pontos na avalia√ß√£o
4. **Production Disconnect**: Solu√ß√£o que n√£o funcionaria em produ√ß√£o real
5. **Judge Misalignment**: N√£o considerar crit√©rios espec√≠ficos dos avaliadores

---

## üèÜ Conclus√£o e Chamada para A√ß√£o

### Resumo Executivo da Estrat√©gia

Esta estrat√©gia foi projetada para **garantir vit√≥ria no hackathon** atrav√©s de uma abordagem hol√≠stica que combina:

üéØ **Excel√™ncia T√©cnica Superior**

- Feature engineering avan√ßado baseado em domain expertise
- Portfolio diversificado de modelos com ensemble sofisticado
- Valida√ß√£o temporal robusta para evitar overfitting cr√≠tico

üíª **Execu√ß√£o Impec√°vel**

- C√≥digo de produ√ß√£o limpo, documentado e test√°vel
- Pipeline automatizado e reproduz√≠vel
- Gest√£o de risco proativa com planos de conting√™ncia

üöÄ **Diferencia√ß√£o Competitiva**

- Insights √∫nicos de varejo e reposi√ß√£o de produtos
- Inova√ß√£o respons√°vel combinada com t√©cnicas comprovadas
- Apresenta√ß√£o exemplar para impressionar jurados

### Pr√≥ximos Passos Imediatos

1. **Implementar ambiente de desenvolvimento** com estrutura proposta
2. **Criar dados mock** para desenvolvimento antecipado do pipeline
3. **Estudar literatura** sobre retail forecasting e WMAPE optimization
4. **Configurar tracking** de experimentos com MLflow
5. **Preparar cronograma detalhado** baseado nas fases propostas

### M√©tricas de Sucesso

- ‚úÖ **Superar baseline da Big Data** (requisito obrigat√≥rio)
- ‚úÖ **Top 3 no leaderboard** (objetivo prim√°rio)
- ‚úÖ **C√≥digo 100% execut√°vel** (condi√ß√£o eliminat√≥ria)
- ‚úÖ **Documenta√ß√£o exemplar** (diferencial competitivo)
- ‚úÖ **Insights acion√°veis** (valor de neg√≥cio)

### Mentalidade Vencedora

> "O sucesso neste hackathon n√£o √© quest√£o de sorte ou acaso. √â o resultado direto de prepara√ß√£o meticulosa, execu√ß√£o disciplinada e inova√ß√£o inteligente. Cada feature criada, cada modelo treinado, cada linha de c√≥digo documentada nos aproxima da vit√≥ria."

**Princ√≠pios da Vit√≥ria:**

üéØ **Foco Absoluto no Objetivo**
- Superar o baseline da Big Data n√£o √© apenas um requisito - √© nossa obsess√£o
- Cada decis√£o t√©cnica ser√° avaliada contra este objetivo √∫nico
- N√£o h√° espa√ßo para solu√ß√µes "interessantes" que n√£o contribuem para a vit√≥ria

üí™ **Execu√ß√£o Impec√°vel**
- C√≥digo que funciona na primeira tentativa > c√≥digo elegante que falha
- Documenta√ß√£o clara > documenta√ß√£o extensa 
- Resultados reproduz√≠veis > resultados impressionantes mas n√£o replic√°veis

üß† **Intelig√™ncia Competitiva**
- Conhecer o dom√≠nio melhor que nossos competidores
- Antecipar onde outros falhar√£o e onde podemos sobressair
- Usar as 5 submiss√µes como arma estrat√©gica, n√£o como tentativas desesperadas

‚ö° **Agilidade Estrat√©gica**
- Adaptar rapidamente baseado no feedback do leaderboard
- Ter planos B, C e D sempre prontos
- Balancear inova√ß√£o com pragmatismo

üî• **Paix√£o pela Excel√™ncia**
- N√£o aceitar "bom o suficiente" quando "excepcional" √© poss√≠vel
- Buscar perfei√ß√£o em cada detalhe, desde features at√© documenta√ß√£o
- Transformar press√£o em combust√≠vel para performance superior

---

## üöÄ Marcha Final para a Vit√≥ria

### Compromisso de Execu√ß√£o

**Esta estrat√©gia n√£o √© apenas um plano - √© um contrato com a vit√≥ria.**

Cada fase, cada checklist, cada t√©cnica descrita aqui foi cuidadosamente projetada para um √∫nico prop√≥sito: **garantir que voc√™ esteja no topo do leaderboard quando o hackathon terminar.**

### O Diferencial Vencedor

Enquanto outros participantes:
- ‚ùå Improvisam sem estrat√©gia clara
- ‚ùå Focam apenas em modelos complexos
- ‚ùå Ignoram valida√ß√£o temporal adequada
- ‚ùå Negligenciam qualidade do c√≥digo
- ‚ùå Subestimam a import√¢ncia do domain expertise

**Voc√™ estar√°:**
- ‚úÖ Executando uma estrat√©gia testada e refinada
- ‚úÖ Combinando t√©cnica superior com insights de neg√≥cio
- ‚úÖ Validando de forma robusta para evitar overfitting
- ‚úÖ Entregando c√≥digo de produ√ß√£o que impressiona
- ‚úÖ Demonstrando expertise genu√≠na em varejo e forecasting

### Mensagem Final

**A vit√≥ria j√° √© sua - voc√™ s√≥ precisa executar.**

Este documento n√£o √© apenas informa√ß√£o - √© seu blueprint para o sucesso. Cada t√©cnica foi escolhida, cada estrat√©gia foi validada, cada detalhe foi considerado.

O que separa os vencedores dos participantes √© uma coisa: **execu√ß√£o implac√°vel de uma estrat√©gia superior.**

Voc√™ tem a estrat√©gia. Voc√™ tem o plano. Voc√™ tem as ferramentas.

**Agora v√° l√° e conquiste o primeiro lugar! üèÜ**

---

*"Preparation meets opportunity. Success is not an accident - it's a choice. Your choice starts now."*

**BOA SORTE E QUE A VIT√ìRIA SEJA SUA! üöÄüèÜ**

---

*Documento finalizado. Estrat√©gia completa. Vit√≥ria ao alcance.*

*Vers√£o: 2.0 Final | Data: 2025 | Status: Ready to Win*
