# ğŸ† EstratÃ©gia Definitiva para Vencer o Hackathon Forecast Big Data 2025

## ğŸ“‹ VisÃ£o Geral Executiva

### Objetivo Central

Desenvolver uma soluÃ§Ã£o de **forecast de vendas superiora ao baseline da Big Data** atravÃ©s de uma combinaÃ§Ã£o estratÃ©gica de:

- **Feature engineering sofisticado** baseado em domain expertise de varejo
- **Portfolio diversificado de modelos** com ensemble inteligente
- **ValidaÃ§Ã£o temporal robusta** para evitar overfitting
- **ExecuÃ§Ã£o tÃ©cnica impecÃ¡vel** com cÃ³digo reproduzÃ­vel e documentado

### Contexto do Problema

- **DomÃ­nio**: Varejo - ReposiÃ§Ã£o de produtos (One-Click Order)
- **Target**: Quantidade semanal de vendas por PDV/SKU
- **PerÃ­odo**: 4 semanas de janeiro/2023
- **Dados**: 1 ano histÃ³rico (2022) + cadastros
- **MÃ©trica**: WMAPE (Weighted Mean Absolute Percentage Error)
- **Benchmark crÃ­tico**: Superar algoritmo interno da Big Data

### Fatores CrÃ­ticos de Sucesso

1. ğŸ¯ **Superar baseline da Big Data** - Requisito obrigatÃ³rio
2. ğŸ’» **CÃ³digo executÃ¡vel** - CondiÃ§Ã£o eliminatÃ³ria
3. ğŸ“Š **Feature engineering superior** - Principal diferencial
4. ğŸ¤– **Ensemble robusto** - ReduÃ§Ã£o de variÃ¢ncia
5. ğŸ“ **DocumentaÃ§Ã£o exemplar** - Impressionar jurados

---

## ğŸ” AnÃ¡lise de Baseline e Benchmarking

### Entendimento do Baseline da Big Data

- **HipÃ³tese**: Provavelmente usa modelos tradicionais (ARIMA, Exponential Smoothing, Linear Regression)
- **LimitaÃ§Ãµes esperadas**:
  - Features bÃ¡sicas (lags simples, mÃ©dias mÃ³veis)
  - Tratamento limitado de sazonalidade complexa
  - Sem ensemble sofisticado
  - Pouco tratamento de casos especiais (cold start, intermittency)

### EstratÃ©gia de SuperaÃ§Ã£o

- **Feature engineering avanÃ§ado**: Features que modelos simples nÃ£o conseguem capturar
- **Ensemble diversificado**: CombinaÃ§Ã£o de mÃºltiplas abordagens
- **Tratamento especializado**: Cold start, produtos intermitentes, sazonalidade complexa
- **Domain expertise**: Insights especÃ­ficos de varejo e reposiÃ§Ã£o

### Como Medir Progresso

- **MÃ©tricas intermediÃ¡rias**: WMAPE por segmento (produto, PDV, categoria)
- **ValidaÃ§Ã£o temporal**: Performance consistente em mÃºltiplos perÃ­odos
- **AnÃ¡lise de erros**: Entender onde estamos superando mÃ©todos tradicionais

---

## ğŸ’¡ Context One-Click Order & Domain Expertise

### Entendimento do NegÃ³cio

- **One-Click Order**: Sistema automatizado de reposiÃ§Ã£o de produtos
- **Objetivo**: Evitar rupturas de estoque mantendo nÃ­veis Ã³timos
- **Desafios tÃ­picos**:
  - Produtos com demanda intermitente
  - Sazonalidade complexa (semanal, mensal, eventos)
  - DiferenÃ§as regionais e por tipo de PDV
  - Produtos novos sem histÃ³rico

### Insights de DomÃ­nio para Features

- **Ciclos de reposiÃ§Ã£o**: FrequÃªncia tÃ­pica de pedidos por produto
- **Complementaridade**: Produtos vendidos em conjunto
- **SubstituiÃ§Ã£o**: Produtos que competem entre si
- **Lifecycle**: Fase do produto (lanÃ§amento, maduro, descontinuado)
- **Regional patterns**: DiferenÃ§as por regiÃ£o/tipo de estabelecimento

---

## ğŸ› ï¸ EstratÃ©gia de Dados Mock para Desenvolvimento

### Necessidade de Dados SintÃ©ticos

- **Problema**: Dados reais sÃ³ disponÃ­veis no inÃ­cio do hackathon
- **SoluÃ§Ã£o**: Criar dados sintÃ©ticos realistas para desenvolvimento antecipado

### Estrutura de Dados Mock Detalhada

```python
# TransaÃ§Ãµes sintÃ©ticas (realistas)
import numpy as np
import pandas as pd
from scipy import stats

# ParÃ¢metros realistas
n_pdvs = 300
n_produtos = 2000
n_weeks = 52

# GeraÃ§Ã£o com padrÃµes realistas
def create_realistic_sales():
    # Base seasonality patterns
    seasonal_weekly = np.sin(2 * np.pi * np.arange(52) / 52)
    seasonal_monthly = np.sin(2 * np.pi * np.arange(52) / 13)
    
    # Product categories with different behaviors
    categories = {
        'beverages': {'base_volume': 100, 'seasonality_strength': 1.2},
        'snacks': {'base_volume': 80, 'seasonality_strength': 0.8},
        'cigarettes': {'base_volume': 150, 'seasonality_strength': 0.3},
        'grocery': {'base_volume': 60, 'seasonality_strength': 1.5}
    }
    
    # PDV types with different characteristics
    pdv_types = {
        'c-store': {'multiplier': 1.0, 'volatility': 0.3},
        'g-store': {'multiplier': 1.8, 'volatility': 0.2},
        'liquor': {'multiplier': 0.7, 'volatility': 0.5}
    }
    
    return generate_synthetic_transactions()

# Cadastro de produtos sintÃ©tico (detalhado)
products_mock = {
    'produto': range(1, 2001),
    'categoria': np.random.choice(['beverages', 'snacks', 'cigarettes', 'grocery'], 2000),
    'subcategoria': generate_subcategories(),
    'marca': generate_brands(),
    'preco_medio': stats.lognorm.rvs(s=0.5, scale=10, size=2000),
    'lifecycle_stage': np.random.choice(['launch', 'growth', 'mature', 'decline'], 2000)
}

# Cadastro de PDVs sintÃ©tico (detalhado)
pdvs_mock = {
    'pdv': range(1001, 1301),
    'tipo': np.random.choice(['c-store', 'g-store', 'liquor'], 300, p=[0.6, 0.3, 0.1]),
    'on_off_prem': np.random.choice(['on', 'off'], 300, p=[0.7, 0.3]),
    'zipcode': generate_zipcodes(),
    'regiao': generate_regions(),
    'size_tier': np.random.choice(['small', 'medium', 'large'], 300, p=[0.5, 0.3, 0.2])
}
```

### Pipeline de Testes

- **Desenvolvimento**: Usar dados mock para criar pipeline completo
- **ValidaÃ§Ã£o**: Testar todos os componentes com dados sintÃ©ticos
- **TransiÃ§Ã£o**: Switch rÃ¡pido para dados reais quando disponÃ­veis

---

## ğŸ¯ Fase 1: PreparaÃ§Ã£o e Setup EstratÃ©gico (Expandido)

### 1.1 Entendimento Profundo do Problema

- **Analisar WMAPE**: Entender como a mÃ©trica penaliza diferentes tipos de erro
- **Estudar literatura**: Papers sobre retail forecasting e demand planning
- **Benchmarking**: Estudar soluÃ§Ãµes existentes (Prophet, DeepAR, etc.)
- **Domain research**: Entender padrÃµes tÃ­picos de varejo

### 1.2 AnÃ¡lise de Recursos Computacionais (Expandida)

#### AvaliaÃ§Ã£o de Hardware
```python
# Resource Assessment
resource_levels = {
    'minimal': {
        'ram': '< 8GB',
        'cpu': '< 4 cores',
        'gpu': None,
        'strategy': 'lightweight_models_only',
        'models': ['LinearRegression', 'Prophet', 'Simple ARIMA'],
        'max_features': 50,
        'ensemble': False
    },
    'standard': {
        'ram': '8-16GB',
        'cpu': '4-8 cores', 
        'gpu': 'Optional',
        'strategy': 'balanced_approach',
        'models': ['LightGBM', 'XGBoost', 'Prophet', 'LSTM (small)'],
        'max_features': 200,
        'ensemble': 'simple_averaging'
    },
    'optimal': {
        'ram': '16GB+',
        'cpu': '8+ cores',
        'gpu': 'Available',
        'strategy': 'full_pipeline',
        'models': ['All models', 'Complex ensembles'],
        'max_features': 'unlimited',
        'ensemble': 'advanced_stacking'
    }
}
```

#### Cloud Strategy Detalhada
- **Free tiers**: Google Colab Pro, Kaggle Kernels, AWS Free Tier
- **Paid options**: AWS EC2, Google Cloud, Azure ML
- **Cost optimization**: Spot instances, preemptible VMs
- **Data storage**: S3, Google Cloud Storage para datasets grandes
- **Backup compute**: Local + cloud hybrid approach

### 1.3 Setup do Ambiente de Desenvolvimento (Detalhado)

```
projeto_hackathon/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/           # Dados originais
â”‚   â”œâ”€â”€ processed/     # Dados limpos
â”‚   â”œâ”€â”€ features/      # Features engineered
â”‚   â””â”€â”€ mock/          # Dados sintÃ©ticos para testes
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_eda/        # AnÃ¡lise exploratÃ³ria
â”‚   â”œâ”€â”€ 02_features/   # Feature engineering
â”‚   â”œâ”€â”€ 03_models/     # Experimentos de modelagem
â”‚   â””â”€â”€ 04_ensemble/   # Ensemble e validaÃ§Ã£o
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data/          # Data processing
â”‚   â”œâ”€â”€ features/      # Feature engineering
â”‚   â”œâ”€â”€ models/        # Model implementations
â”‚   â””â”€â”€ utils/         # Utilities
â”œâ”€â”€ models/            # Saved models
â”œâ”€â”€ submissions/       # Prediction files
â”œâ”€â”€ docs/             # Documentation
â”œâ”€â”€ tests/            # Unit tests
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ Makefile
â””â”€â”€ README.md
```

### 1.4 Versionamento e Tracking

- **Git strategy**: Branches por experimento
- **MLflow setup**: Tracking de experimentos
- **Config management**: Hydra ou similar
- **Results logging**: Structured logging para todas as runs

---

## ğŸ” Fase 2: AnÃ¡lise ExploratÃ³ria EstratÃ©gica (Expandido)

### 2.1 AnÃ¡lise Temporal Profunda

- **PadrÃµes semanais**: Segunda vs fim de semana
- **Sazonalidade mensal**: InÃ­cio vs fim de mÃªs (pagamento)
- **Eventos especiais**: Feriados, promoÃ§Ãµes, eventos regionais
- **Trends de longo prazo**: Crescimento/declÃ­nio por categoria
- **AutocorrelaÃ§Ã£o**: Identificar lags mais relevantes

### 2.2 SegmentaÃ§Ã£o EstratÃ©gica AvanÃ§ada

- **ABC Analysis**: Produtos por volume/faturamento
- **Cluster de comportamento**: Produtos com padrÃµes similares
- **PDV profiling**: SegmentaÃ§Ã£o por performance e caracterÃ­sticas
- **Regional analysis**: PadrÃµes geogrÃ¡ficos (urbano vs rural)
- **Cross-category analysis**: InteraÃ§Ãµes entre categorias

### 2.3 AnÃ¡lise de Qualidade e ConsistÃªncia

- **Missing data patterns**: SistemÃ¡tico vs aleatÃ³rio
- **Outlier detection**: MÃ©todos mÃºltiplos (IQR, Z-score, Isolation Forest)
- **Data drift**: MudanÃ§as de distribuiÃ§Ã£o ao longo do tempo
- **Consistency checks**: ValidaÃ§Ãµes de integridade
- **Coverage analysis**: Completude por dimensÃ£o

### 2.4 Insights EspecÃ­ficos de Varejo

- **Velocity analysis**: Fast vs slow-moving products
- **Seasonality detection**: MÃ©todos automÃ¡ticos (STL, X-13)
- **Cross-selling patterns**: Market basket analysis
- **Price elasticity**: Impacto de preÃ§os nas vendas
- **Promotion effects**: DetecÃ§Ã£o e quantificaÃ§Ã£o

---

## ğŸ› ï¸ Fase 3: Feature Engineering AvanÃ§ado (Expandido)

### 3.1 Features Temporais Sofisticadas

```python
# Lags e Windows
- lags: [1, 2, 3, 4, 8, 12, 26, 52] semanas
- rolling_stats: mean, median, std, min, max, skew
- windows: [4, 8, 12, 26] semanas
- exponential_decay: alpha = [0.1, 0.3, 0.5]

# Sazonalidade AvanÃ§ada
- fourier_components: mÃºltiplos perÃ­odos (4, 13, 26, 52)
- day_of_week: dummy variables
- week_of_month: 1-4
- month_interactions: categoria Ã— mÃªs
- seasonal_strength: magnitude da sazonalidade

# TendÃªncias e Derivadas
- linear_trends: slope em janelas mÃ³veis
- acceleration: segunda derivada
- volatility: rolling std / rolling mean
- momentum: diferenÃ§as percentuais

# Features EspecÃ­ficas para WMAPE
- percentage_error_features: features que minimizam erro percentual
- volume_weighted_features: features ponderadas pelo volume
- relative_performance: performance vs mÃ©dia da categoria
- forecast_difficulty: indicador de dificuldade de previsÃ£o
```

### 3.2 Features de AgregaÃ§Ã£o Inteligentes

```python
# Por Produto
- total_sales_product: soma histÃ³rica
- avg_sales_per_pdv: mÃ©dia por ponto
- product_volatility: CV das vendas
- product_growth_rate: tendÃªncia

# Por PDV
- portfolio_diversity: entropia das categorias
- pdv_total_volume: soma total
- pdv_growth_trajectory: tendÃªncia
- top_products_share: % dos top 10 produtos

# Cross-Features
- product_pdv_affinity: vendas produto no PDV / mÃ©dia
- category_region_share: share da categoria na regiÃ£o
- product_seasonality_match: alinhamento com sazonalidade
```

### 3.3 Features Comportamentais AvanÃ§adas

```python
# IntermitÃªncia
- zero_weeks_ratio: % semanas com venda zero
- consecutive_zeros: mÃ¡ximo de zeros consecutivos
- purchase_frequency: frequÃªncia mÃ©dia de compra
- reorder_cycles: ciclos tÃ­picos de recompra

# Lifecycle
- weeks_since_first_sale: idade do produto
- growth_stage: classificaÃ§Ã£o automÃ¡tica
- maturity_indicator: estabilidade das vendas
- decline_indicator: detecÃ§Ã£o de declÃ­nio

# Market Dynamics
- market_share_product: % do produto na categoria
- competitive_pressure: nÃºmero de substitutos
- cross_selling_strength: forÃ§a das associaÃ§Ãµes
- cannibalization_risk: competiÃ§Ã£o interna
```

### 3.4 Features de Contexto Externo

```python
# GeogrÃ¡ficos
- zipcode_density: PDVs por regiÃ£o
- urban_rural_indicator: classificaÃ§Ã£o da Ã¡rea
- economic_index: se disponÃ­vel, Ã­ndices socioeconÃ´micos
- distance_to_competitors: proximidade de concorrentes

# Temporais Externos
- holiday_proximity: distÃ¢ncia de feriados
- payroll_calendar: proximity to typical payday
- school_calendar: inÃ­cio/fim de perÃ­odo letivo
- weather_seasonality: se aplicÃ¡vel
```

---

## ğŸ¤– Fase 4: EstratÃ©gia de Modelagem AvanÃ§ada (Expandido)

### 4.1 Portfolio de Modelos Diversificado

#### Time Series Models

```python
# Prophet
- seasonalities: weekly, monthly, yearly
- holidays: custom holiday calendar
- changepoint_detection: automatic trend changes
- mcmc_samples: uncertainty quantification

# ARIMA/SARIMA
- automated selection: auto_arima
- seasonal_decompose: STL decomposition
- residual_analysis: diagnostic plots
- confidence_intervals: prediction uncertainty

# State Space Models
- exponential_smoothing: Holt-Winters
- ets_models: automated ETS selection
- structural_time_series: trend + seasonal components
```

#### Tree-Based Models

```python
# LightGBM
- categorical_feature: automatic handling
- early_stopping: overfitting prevention
- feature_importance: SHAP values
- hyperparameter_tuning: Optuna

# XGBoost
- objective: reg:squarederror
- eval_metric: wmape (custom)
- tree_method: gpu_hist if available
- regularization: alpha, lambda tuning

# CatBoost
- cat_features: automatic detection
- overfitting_detector: built-in
- feature_interactions: automatic
- uncertainty: virtual_ensembles
```

#### Neural Networks

```python
# LSTM/GRU
- sequence_length: [12, 26, 52] weeks
- attention_mechanism: temporal attention
- bidirectional: capture future context
- dropout: regularization

# Transformer
- positional_encoding: temporal positions
- multi_head_attention: multiple patterns
- encoder_decoder: seq2seq architecture
- pre_training: if applicable

# Neural Prophet
- trend_components: automatic detection
- seasonality_components: learnable
- event_modeling: promotional effects
- uncertainty_quantification: built-in
```

### 4.2 ValidaÃ§Ã£o Temporal Robusta (Detalhada)

#### Time Series Cross-Validation

```python
# Walk-Forward Validation
- initial_train: 40 weeks
- forecast_horizon: 4 weeks
- step_size: 1 week
- n_splits: 8-10 splits

# Blocked Cross-Validation
- block_size: 4 weeks
- gap_size: 2 weeks (simulate data delay)
- purge_size: 1 week (avoid leakage)
- embargo: 1 week (trading-like validation)
```

#### Metrics Tracking

```python
# Primary Metric
- wmape: weighted mean absolute percentage error
- wmape_by_segment: por produto, PDV, categoria
- consistency: performance across time periods

# Secondary Metrics
- mae: mean absolute error
- rmse: root mean squared error
- mape: mean absolute percentage error
- directional_accuracy: trend prediction
```

### 4.3 Tratamento de Casos Especiais (Detalhado)

#### Cold Start Problem

```python
# New Products
- similarity_based: find similar products
- category_averages: use category patterns
- hierarchical_forecasting: top-down approach
- collaborative_filtering: user-item matrix

# New PDVs
- demographic_matching: similar PDV profiles
- geographic_clustering: regional patterns
- size_based_scaling: volume adjustments
```

#### Intermittent Demand

```python
# Zero-Inflated Models (Expandido)
- hurdle_models: binary + continuous
  * Stage 1: Predict probability of non-zero sales
  * Stage 2: Predict quantity given non-zero
  * Use different features for each stage
  
- zero_inflated_regression: two-part model
  * Logistic regression for zero/non-zero
  * Count regression for positive values
  * Handle overdispersion with negative binomial
  
- croston_method: specialized for intermittent
  * Simple exponential smoothing on non-zero values
  * Separate smoothing for inter-arrival times
  * SBA (Syntetos-Boylan) adjustment for bias
  
- bootstrap_sampling: uncertainty from zeros
  * Resample historical zero patterns
  * Generate prediction intervals
  * Account for zero inflation in metrics
  
# Advanced Intermittency Features
- intermittency_ratio: zeros / total_observations
- average_inter_demand_interval: avg weeks between sales
- demand_intensity: avg demand when > 0
- zero_streak_analysis: consecutive zero patterns
- reactivation_probability: likelihood of returning to positive sales
```

#### Seasonal Products

```python
# Strong Seasonality
- seasonal_decomposition: trend + seasonal + residual
- harmonic_regression: fourier series
- seasonal_naive: simple seasonal patterns
- regime_switching: different models per season
```

---

## ğŸ”¬ Fase 5: OtimizaÃ§Ã£o e Refinamento (Expandido)

### 5.1 AnÃ¡lise de Erros Detalhada

#### Error Decomposition

```python
# By Dimensions
- error_by_product: identify problematic products
- error_by_pdv: problematic locations
- error_by_time: temporal patterns in errors
- error_by_volume: small vs large sales

# Error Patterns
- systematic_bias: consistent over/under prediction
- heteroskedasticity: variance changes
- autocorrelation: error correlation over time
- seasonal_errors: seasonal bias patterns
```

#### Residual Analysis

```python
# Statistical Tests
- ljung_box: autocorrelation in residuals
- jarque_bera: normality of residuals
- arch_test: heteroskedasticity
- runs_test: randomness

# Visual Diagnostics
- qq_plots: normality assessment
- residual_vs_fitted: homoskedasticity
- acf_pacf_plots: autocorrelation
- seasonal_plots: seasonal residuals
```

### 5.2 Model Calibration (Detalhado)

#### Probability Calibration

```python
# Calibration Methods
- platt_scaling: logistic regression
- isotonic_regression: monotonic mapping
- temperature_scaling: neural networks
- conformal_prediction: distribution-free

# Uncertainty Quantification
- prediction_intervals: confidence bounds
- quantile_regression: percentile predictions
- bootstrap_sampling: empirical distribution
- bayesian_methods: posterior sampling
```

### 5.3 Ensemble Strategy (AvanÃ§ado)

#### Stacking Architecture

```python
# Level 1 Models (Base Learners)
- prophet: seasonal patterns
- lightgbm: feature interactions
- lstm: temporal dependencies
- arima: time series structure

# Level 2 Model (Meta-Learner)
- linear_regression: simple combination
- ridge_regression: regularized combination
- neural_network: non-linear combination
- feature_based: conditional weighting
```

#### Dynamic Weighting

```python
# Conditional Ensemble
- product_based_weights: different weights per product
- time_based_weights: seasonal weight adjustments
- performance_based_weights: based on recent accuracy
- uncertainty_based_weights: confidence-weighted averaging
```

### 5.4 Post-Processing (Business Rules)

#### Business Constraints

```python
# Inventory Constraints
- minimum_order_quantity: MOQ constraints
- maximum_capacity: storage limitations
- integer_constraints: whole units only
- non_negativity: no negative predictions

# Business Logic
- demand_smoothing: avoid extreme fluctuations
- promotional_adjustments: known promotions
- lifecycle_adjustments: product phase considerations
- competitive_adjustments: market share constraints
```

---

## ğŸ’» Fase 6: Qualidade TÃ©cnica e ExecuÃ§Ã£o (Expandido)

### 6.1 CÃ³digo de ProduÃ§Ã£o (Detalhado)

#### Architecture Patterns

```python
# Design Patterns
- factory_pattern: model creation
- strategy_pattern: different algorithms
- observer_pattern: logging and monitoring
- pipeline_pattern: data flow

# Code Structure
src/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ loaders.py      # Data loading utilities
â”‚   â”œâ”€â”€ preprocessors.py # Data cleaning
â”‚   â””â”€â”€ validators.py   # Data validation
â”œâ”€â”€ features/
â”‚   â”œâ”€â”€ temporal.py     # Time-based features
â”‚   â”œâ”€â”€ aggregates.py   # Aggregation features
â”‚   â””â”€â”€ categorical.py  # Category features
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ base.py        # Abstract base class
â”‚   â”œâ”€â”€ timeseries.py  # Time series models
â”‚   â”œâ”€â”€ ml_models.py   # ML models
â”‚   â””â”€â”€ ensemble.py    # Ensemble methods
â”œâ”€â”€ evaluation/
â”‚   â”œâ”€â”€ metrics.py     # Custom metrics
â”‚   â”œâ”€â”€ validation.py  # Cross-validation
â”‚   â””â”€â”€ analysis.py    # Error analysis
â””â”€â”€ utils/
    â”œâ”€â”€ config.py      # Configuration management
    â”œâ”€â”€ logging.py     # Logging utilities
    â””â”€â”€ helpers.py     # Helper functions
```

#### Configuration Management

```yaml
# config.yaml
data:
  path: "data/raw/"
  validation_split: 0.2

features:
  lag_features: [1, 2, 3, 4, 8, 12]
  rolling_windows: [4, 8, 12, 26]
  seasonal_periods: [4, 13, 26, 52]

models:
  lightgbm:
    num_leaves: 31
    learning_rate: 0.1
    objective: "regression"

  prophet:
    yearly_seasonality: true
    weekly_seasonality: true
    daily_seasonality: false

ensemble:
  method: "stacking"
  cv_folds: 5
  meta_model: "ridge"

evaluation:
  primary_metric: "wmape"
  cv_method: "time_series_split"
  n_splits: 8
```

### 6.2 Pipeline Automatizado (Completo)

#### MLOps Pipeline

```python
# Makefile
.PHONY: data features models ensemble evaluate submit

data:
    python src/data/load_data.py
    python src/data/clean_data.py
    python src/data/validate_data.py

features:
    python src/features/create_temporal_features.py
    python src/features/create_aggregate_features.py
    python src/features/create_categorical_features.py

models:
    python src/models/train_prophet.py
    python src/models/train_lightgbm.py
    python src/models/train_lstm.py

ensemble:
    python src/models/train_ensemble.py
    python src/evaluation/validate_ensemble.py

evaluate:
    python src/evaluation/comprehensive_evaluation.py
    python src/evaluation/generate_report.py

submit:
    python src/models/generate_predictions.py
    python src/utils/format_submission.py
```

### 6.3 Testing Strategy

```python
# tests/
â”œâ”€â”€ test_data/
â”‚   â”œâ”€â”€ test_loaders.py
â”‚   â”œâ”€â”€ test_preprocessors.py
â”‚   â””â”€â”€ test_validators.py
â”œâ”€â”€ test_features/
â”‚   â”œâ”€â”€ test_temporal.py
â”‚   â”œâ”€â”€ test_aggregates.py
â”‚   â””â”€â”€ test_categorical.py
â”œâ”€â”€ test_models/
â”‚   â”œâ”€â”€ test_base.py
â”‚   â”œâ”€â”€ test_timeseries.py
â”‚   â””â”€â”€ test_ensemble.py
â””â”€â”€ test_utils/
    â”œâ”€â”€ test_metrics.py
    â””â”€â”€ test_helpers.py

# pytest configuration
pytest.ini:
[tool:pytest]
testpaths = tests
python_files = test_*.py
python_classes = Test*
python_functions = test_*
addopts = -v --tb=short --strict-markers
```

---

## ğŸ“Š MÃ©tricas de ValidaÃ§Ã£o e KPIs IntermediÃ¡rios

### 6.4 Monitoring Dashboard

```python
# MLflow Tracking
import mlflow

# Track experiments
with mlflow.start_run():
    mlflow.log_param("model", "ensemble")
    mlflow.log_metric("wmape_cv", cv_wmape)
    mlflow.log_metric("wmape_by_product", product_wmape)
    mlflow.log_artifact("feature_importance.png")
    mlflow.sklearn.log_model(model, "model")

# Validation Metrics
validation_metrics = {
    'primary': {
        'wmape_overall': float,
        'wmape_by_segment': dict,
        'consistency_score': float
    },
    'secondary': {
        'mae': float,
        'rmse': float,
        'directional_accuracy': float
    },
    'business': {
        'coverage_95': float,  # % predictions within 95% CI
        'bias_by_volume': dict,  # bias for different volume tiers
        'seasonal_accuracy': dict  # accuracy by season
    }
}
```

---

## ğŸ¯ Fase 7: EstratÃ©gia de SubmissÃµes (Refinada)

### 7.1 SubmissÃ£o Strategy (Detalhada)

#### Timeline Strategy

```python
# 5 Submissions Plan
submissions = {
    1: {
        'timing': 'Day 3',
        'model': 'Simple baseline (Prophet)',
        'purpose': 'Test pipeline and get initial score',
        'risk': 'Low',
        'expected_wmape': 'High (baseline)'
    },
    2: {
        'timing': 'Day 7',
        'model': 'Best single model (LightGBM)',
        'purpose': 'Validate feature engineering',
        'risk': 'Medium',
        'expected_wmape': 'Medium'
    },
    3: {
        'timing': 'Day 10',
        'model': 'Initial ensemble',
        'purpose': 'Test ensemble approach',
        'risk': 'Medium',
        'expected_wmape': 'Lower'
    },
    4: {
        'timing': 'Day 13',
        'model': 'Optimized ensemble',
        'purpose': 'Leaderboard-informed optimization',
        'risk': 'Medium-High',
        'expected_wmape': 'Low'
    },
    5: {
        'timing': 'Final day',
        'model': 'Final ensemble + post-processing',
        'purpose': 'Last optimization push',
        'risk': 'High',
        'expected_wmape': 'Lowest'
    }
}
```

### 7.2 Leaderboard Analysis Strategy

```python
# Competitive Intelligence
def analyze_competition():
    # Score gaps analysis
    score_gaps = calculate_gaps_to_top_positions()

    # Improvement needed
    improvement_needed = {
        'top_3': score_gaps['top_3'] * 1.05,  # 5% buffer
        'top_10': score_gaps['top_10'] * 1.05,
        'beat_baseline': baseline_score * 0.95  # 5% better than baseline
    }

    # Strategic decisions
    if improvement_needed['top_3'] < 2.0:  # if close to top 3
        strategy = 'aggressive_optimization'
    elif improvement_needed['beat_baseline'] > 10.0:  # far from baseline
        strategy = 'fundamental_improvements'
    else:
        strategy = 'incremental_optimization'

    return strategy
```

### 7.3 Risk Management

```python
# Submission Risk Assessment
def assess_submission_risk(model, validation_score):
    risk_factors = {
        'overfitting_risk': calculate_train_val_gap(model),
        'complexity_risk': assess_model_complexity(model),
        'data_leakage_risk': check_for_leakage(model),
        'execution_risk': test_prediction_pipeline(model)
    }

    overall_risk = weighted_average(risk_factors)

    if overall_risk > 0.7:
        return 'HIGH_RISK - Consider simpler model'
    elif overall_risk > 0.4:
        return 'MEDIUM_RISK - Additional validation needed'
    else:
        return 'LOW_RISK - Safe to submit'
```

---

## âš ï¸ Contingency Planning

### 8.1 Scenario Planning

#### Scenario A: Hardware Limitations

```python
# Lightweight Model Strategy
contingency_models = {
    'low_memory': {
        'model': 'LinearRegression + feature selection',
        'features': 'top_50_features',
        'training_time': '< 30 minutes'
    },
    'medium_compute': {
        'model': 'LightGBM with reduced features',
        'features': 'top_200_features',
        'training_time': '< 2 hours'
    },
    'full_compute': {
        'model': 'Full ensemble',
        'features': 'all_features',
        'training_time': '< 12 hours'
    }
}
```

#### Scenario B: Data Quality Issues

```python
# Data Quality Contingency
data_issues_response = {
    'missing_data': {
        'strategy': 'robust_imputation',
        'methods': ['median', 'mode', 'interpolation'],
        'validation': 'impact_analysis'
    },
    'outliers': {
        'strategy': 'robust_models',
        'methods': ['clip', 'winsorize', 'robust_scaling'],
        'models': 'tree_based_preferred'
    },
    'data_drift': {
        'strategy': 'temporal_validation',
        'methods': ['adaptive_models', 'recent_data_weighting'],
        'monitoring': 'distribution_tracking'
    }
}
```

#### Scenario C: Model Performance Issues

```python
# Performance Contingency
if model_performance < baseline_threshold:
    # Diagnostic checklist
    diagnostics = {
        'feature_importance': 'check_feature_relevance',
        'overfitting': 'check_train_val_gap',
        'data_leakage': 'check_temporal_integrity',
        'model_selection': 'try_simpler_models'
    }

    # Recovery actions
    recovery_actions = {
        'feature_engineering': 'simplify_features',
        'model_complexity': 'reduce_complexity',
        'validation': 'more_robust_cv',
        'ensemble': 'fewer_base_models'
    }
```

---

## ğŸ¨ Interpretabilidade e Explicabilidade

### 9.1 Model Interpretability Strategy

```python
# Interpretability Tools
interpretability_suite = {
    'global': {
        'feature_importance': 'SHAP, permutation importance',
        'partial_dependence': 'PDPs for key features',
        'interaction_effects': 'H-statistics, SHAP interactions'
    },
    'local': {
        'instance_explanation': 'LIME, SHAP values',
        'counterfactual': 'what-if scenarios',
        'prototype_examples': 'representative instances'
    },
    'temporal': {
        'time_series_decomposition': 'trend + seasonal + residual',
        'attribution_over_time': 'SHAP temporal plots',
        'regime_detection': 'changepoint analysis'
    }
}
```

### 9.2 Business Insights Generation

```python
# Actionable Insights
business_insights = {
    'product_insights': {
        'top_drivers': 'most important features per product',
        'seasonal_patterns': 'seasonal behavior by product',
        'cross_selling': 'product association rules',
        'lifecycle_stage': 'product maturity analysis'
    },
    'pdv_insights': {
        'performance_drivers': 'what makes PDVs successful',
        'geographic_patterns': 'regional differences',
        'portfolio_optimization': 'optimal product mix',
        'capacity_utilization': 'volume vs performance'
    },
    'temporal_insights': {
        'peak_seasons': 'high demand periods',
        'trend_analysis': 'growth/decline patterns',
        'anomaly_detection': 'unusual events',
        'forecasting_accuracy': 'prediction confidence by time'
    }
}
```

---

## ğŸ”„ AnÃ¡lise Competitiva e DiferenciaÃ§Ã£o

### 10.1 Competitive Analysis

```python
# Differentiation Strategy
differentiation_factors = {
    'technical': {
        'advanced_features': 'novel feature engineering',
        'ensemble_sophistication': 'multi-level stacking',
        'uncertainty_quantification': 'confidence intervals',
        'robustness': 'multiple validation schemes'
    },
    'business': {
        'domain_expertise': 'retail-specific insights',
        'interpretability': 'actionable business insights',
        'scalability': 'production-ready code',
        'documentation': 'comprehensive explanation'
    },
    'innovation': {
        'novel_approaches': 'unique techniques',
        'creative_features': 'domain-inspired features',
        'ensemble_creativity': 'innovative combinations',
        'post_processing': 'business-aware adjustments'
    }
}
```

### 10.2 Presentation Strategy

```python
# Judging Criteria Alignment
presentation_strategy = {
    'technical_excellence': {
        'code_quality': 'clean, documented, testable',
        'methodology': 'rigorous, well-validated',
        'innovation': 'creative but sound approaches',
        'reproducibility': 'fully executable pipeline'
    },
    'business_impact': {
        'practical_value': 'real-world applicability',
        'insights': 'actionable recommendations',
        'scalability': 'production considerations',
        'roi_potential': 'business value demonstration'
    },
    'communication': {
        'clarity': 'clear explanation of approach',
        'storytelling': 'coherent narrative',
        'visualization': 'effective charts and plots',
        'executive_summary': 'concise key points'
    }
}
```

---

## âš¡ Cronograma Executivo Detalhado

### Phase-by-Phase Timeline (14 dias)

#### Week 1: Foundation and Exploration

```python
# Days 1-2: Setup and EDA
day_1 = {
    'morning': 'Environment setup, data loading, initial exploration',
    'afternoon': 'Data quality assessment, missing value analysis',
    'evening': 'Basic visualizations, initial insights'
}

day_2 = {
    'morning': 'Temporal analysis, seasonality detection',
    'afternoon': 'Segmentation analysis, clustering',
    'evening': 'Feature ideas brainstorming, documentation'
}

# Days 3-4: Feature Engineering
day_3 = {
    'morning': 'Temporal features creation',
    'afternoon': 'Aggregate features, cross-features',
    'evening': 'Feature validation, first baseline model',
    'milestone': 'SUBMISSION 1 - Simple baseline'
}

day_4 = {
    'morning': 'Advanced features, behavioral features',
    'afternoon': 'Feature selection, importance analysis',
    'evening': 'Feature pipeline optimization'
}

# Days 5-7: Core Modeling
day_5 = {
    'morning': 'Prophet model development',
    'afternoon': 'LightGBM model development',
    'evening': 'LSTM model experimentation',
    'milestone': 'First competitive model trained'
}

day_6 = {
    'morning': 'Model validation, hyperparameter tuning',
    'afternoon': 'Cross-validation setup, performance analysis',
    'evening': 'Model comparison, best single model selection'
}

day_7 = {
    'morning': 'Model refinement, feature importance analysis',
    'afternoon': 'Error analysis, model diagnostics',
    'evening': 'Second submission preparation',
    'milestone': 'SUBMISSION 2 - Best single model'
}
```

#### Week 2: Optimization and Finalization

```python
# Days 8-10: Ensemble and Optimization
day_8 = {
    'morning': 'Ensemble architecture design',
    'afternoon': 'Stacking implementation, meta-model training',
    'evening': 'Ensemble validation, performance analysis',
    'milestone': 'Ensemble model working'
}

day_9 = {
    'morning': 'Ensemble optimization, weight tuning',
    'afternoon': 'Post-processing implementation',
    'evening': 'Business rules integration, constraint handling'
}

day_10 = {
    'morning': 'Final ensemble validation',
    'afternoon': 'Submission preparation, error analysis',
    'evening': 'Third submission, leaderboard analysis',
    'milestone': 'SUBMISSION 3 - Initial ensemble'
}

# Days 11-12: Quality and Polish
day_11 = {
    'morning': 'Code refactoring, documentation',
    'afternoon': 'Testing, pipeline validation',
    'evening': 'Repository organization, README creation',
    'milestone': 'Code ready for evaluation'
}

day_12 = {
    'morning': 'Interpretability analysis, insight generation',
    'afternoon': 'Presentation notebook creation',
    'evening': 'Executive summary writing'
}

# Days 13-14: Final Push
day_13 = {
    'morning': 'Leaderboard analysis, competitive intelligence',
    'afternoon': 'Final optimization based on standings',
    'evening': 'Fourth submission, risk assessment',
    'milestone': 'SUBMISSION 4 - Optimized model'
}

day_14 = {
    'morning': 'Last-minute improvements, final validation',
    'afternoon': 'Final submission preparation',
    'evening': 'SUBMISSION 5 - Final push',
    'milestone': 'Competition complete'
}
```

### Daily Checkpoints

```python
daily_checklist = {
    'progress_review': 'What was accomplished today?',
    'blocker_identification': 'What obstacles were encountered?',
    'next_day_planning': 'What are tomorrow\'s priorities?',
    'risk_assessment': 'Any new risks identified?',
    'backup_strategy': 'Contingency plans updated?'
}
```

---

## ğŸ–ï¸ Fatores CrÃ­ticos de Sucesso

### Technical Excellence

1. **Feature Engineering Superior**: O principal diferencial competitivo
2. **Robust Validation**: Evitar overfitting atravÃ©s de validaÃ§Ã£o temporal rigorosa
3. **Ensemble Mastery**: Combinar modelos de forma inteligente para reduzir variÃ¢ncia
4. **Edge Case Handling**: Tratamento especial para cold start e intermittent demand
5. **Code Quality**: ExecutÃ¡vel, reproduzÃ­vel, bem documentado

### Strategic Execution

1. **Domain Expertise**: Demonstrar conhecimento profundo do varejo
2. **Competitive Intelligence**: Monitorar e reagir ao leaderboard estrategicamente
3. **Risk Management**: Balancear inovaÃ§Ã£o com execuÃ§Ã£o segura
4. **Resource Optimization**: Maximizar eficiÃªncia computacional e temporal
5. **Presentation Excellence**: Impressionar jurados com qualidade tÃ©cnica

### Innovation Balance

1. **Creative Features**: Features Ãºnicos baseados em domain expertise
2. **Proven Techniques**: Combinar inovaÃ§Ã£o com mÃ©todos comprovados
3. **Explainable AI**: Modelos complexos mas interpretÃ¡veis
4. **Business Value**: Focar em aplicabilidade real, nÃ£o apenas accuracy
5. **Scalable Solution**: Considerar deployment em produÃ§Ã£o

---

## ğŸš¨ Armadilhas CrÃ­ticas a Evitar

### Technical Pitfalls

1. **Data Leakage**: Nunca usar informaÃ§Ã£o futura - validar rigorosamente
2. **Overfitting Temporal**: Sempre usar time-based splits, nunca shuffle
3. **Feature Leakage**: Cuidado com features que nÃ£o estarÃ£o disponÃ­veis na prediÃ§Ã£o
4. **Validation Mismatch**: Garantir que CV simula exatamente o cenÃ¡rio real
5. **Scale Issues**: Verificar se features estÃ£o na escala correta

### Strategic Mistakes

1. **Complexity Trap**: Modelo simples funcionando > modelo complexo quebrado
2. **Deadline Rush**: Deixar tempo adequado para polimento e testes
3. **Single Point of Failure**: Ter sempre planos B funcionando
4. **Documentation Neglect**: DocumentaÃ§Ã£o pobre pode custar pontos
5. **Submission Panic**: Usar as 5 tentativas estrategicamente, nÃ£o desesperadamente

### Business Oversights

1. **Baseline Ignorance**: Focar obsessivamente em superar o benchmark da Big Data
2. **Domain Blindness**: Ignorar conhecimento de varejo especÃ­fico
3. **Interpretability Gap**: Modelo black-box pode perder pontos na avaliaÃ§Ã£o
4. **Production Disconnect**: SoluÃ§Ã£o que nÃ£o funcionaria em produÃ§Ã£o real
5. **Judge Misalignment**: NÃ£o considerar critÃ©rios especÃ­ficos dos avaliadores

---

## ğŸ† ConclusÃ£o e Chamada para AÃ§Ã£o

### Resumo Executivo da EstratÃ©gia

Esta estratÃ©gia foi projetada para **garantir vitÃ³ria no hackathon** atravÃ©s de uma abordagem holÃ­stica que combina:

ğŸ¯ **ExcelÃªncia TÃ©cnica Superior**

- Feature engineering avanÃ§ado baseado em domain expertise
- Portfolio diversificado de modelos com ensemble sofisticado
- ValidaÃ§Ã£o temporal robusta para evitar overfitting crÃ­tico

ğŸ’» **ExecuÃ§Ã£o ImpecÃ¡vel**

- CÃ³digo de produÃ§Ã£o limpo, documentado e testÃ¡vel
- Pipeline automatizado e reproduzÃ­vel
- GestÃ£o de risco proativa com planos de contingÃªncia

ğŸš€ **DiferenciaÃ§Ã£o Competitiva**

- Insights Ãºnicos de varejo e reposiÃ§Ã£o de produtos
- InovaÃ§Ã£o responsÃ¡vel combinada com tÃ©cnicas comprovadas
- ApresentaÃ§Ã£o exemplar para impressionar jurados

### PrÃ³ximos Passos Imediatos

1. **Implementar ambiente de desenvolvimento** com estrutura proposta
2. **Criar dados mock** para desenvolvimento antecipado do pipeline
3. **Estudar literatura** sobre retail forecasting e WMAPE optimization
4. **Configurar tracking** de experimentos com MLflow
5. **Preparar cronograma detalhado** baseado nas fases propostas

### MÃ©tricas de Sucesso

- âœ… **Superar baseline da Big Data** (requisito obrigatÃ³rio)
- âœ… **Top 3 no leaderboard** (objetivo primÃ¡rio)
- âœ… **CÃ³digo 100% executÃ¡vel** (condiÃ§Ã£o eliminatÃ³ria)
- âœ… **DocumentaÃ§Ã£o exemplar** (diferencial competitivo)
- âœ… **Insights acionÃ¡veis** (valor de negÃ³cio)

### Mentalidade Vencedora

> "O sucesso neste hackathon nÃ£o Ã© questÃ£o de sorte ou acaso. Ã‰ o resultado direto de preparaÃ§Ã£o meticulosa, execuÃ§Ã£o disciplinada e inovaÃ§Ã£o inteligente. Cada feature criada, cada modelo treinado, cada linha de cÃ³digo documentada nos aproxima da vitÃ³ria."

**PrincÃ­pios da VitÃ³ria:**

ğŸ¯ **Foco Absoluto no Objetivo**
- Superar o baseline da Big Data nÃ£o Ã© apenas um requisito - Ã© nossa obsessÃ£o
- Cada decisÃ£o tÃ©cnica serÃ¡ avaliada contra este objetivo Ãºnico
- NÃ£o hÃ¡ espaÃ§o para soluÃ§Ãµes "interessantes" que nÃ£o contribuem para a vitÃ³ria

ğŸ’ª **ExecuÃ§Ã£o ImpecÃ¡vel**
- CÃ³digo que funciona na primeira tentativa > cÃ³digo elegante que falha
- DocumentaÃ§Ã£o clara > documentaÃ§Ã£o extensa 
- Resultados reproduzÃ­veis > resultados impressionantes mas nÃ£o replicÃ¡veis

ğŸ§  **InteligÃªncia Competitiva**
- Conhecer o domÃ­nio melhor que nossos competidores
- Antecipar onde outros falharÃ£o e onde podemos sobressair
- Usar as 5 submissÃµes como arma estratÃ©gica, nÃ£o como tentativas desesperadas

âš¡ **Agilidade EstratÃ©gica**
- Adaptar rapidamente baseado no feedback do leaderboard
- Ter planos B, C e D sempre prontos
- Balancear inovaÃ§Ã£o com pragmatismo

ğŸ”¥ **PaixÃ£o pela ExcelÃªncia**
- NÃ£o aceitar "bom o suficiente" quando "excepcional" Ã© possÃ­vel
- Buscar perfeiÃ§Ã£o em cada detalhe, desde features atÃ© documentaÃ§Ã£o
- Transformar pressÃ£o em combustÃ­vel para performance superior

---

## ğŸš€ Marcha Final para a VitÃ³ria

### Compromisso de ExecuÃ§Ã£o

**Esta estratÃ©gia nÃ£o Ã© apenas um plano - Ã© um contrato com a vitÃ³ria.**

Cada fase, cada checklist, cada tÃ©cnica descrita aqui foi cuidadosamente projetada para um Ãºnico propÃ³sito: **garantir que vocÃª esteja no topo do leaderboard quando o hackathon terminar.**

### O Diferencial Vencedor

Enquanto outros participantes:
- âŒ Improvisam sem estratÃ©gia clara
- âŒ Focam apenas em modelos complexos
- âŒ Ignoram validaÃ§Ã£o temporal adequada
- âŒ Negligenciam qualidade do cÃ³digo
- âŒ Subestimam a importÃ¢ncia do domain expertise

**VocÃª estarÃ¡:**
- âœ… Executando uma estratÃ©gia testada e refinada
- âœ… Combinando tÃ©cnica superior com insights de negÃ³cio
- âœ… Validando de forma robusta para evitar overfitting
- âœ… Entregando cÃ³digo de produÃ§Ã£o que impressiona
- âœ… Demonstrando expertise genuÃ­na em varejo e forecasting

### Mensagem Final

**A vitÃ³ria jÃ¡ Ã© sua - vocÃª sÃ³ precisa executar.**

Este documento nÃ£o Ã© apenas informaÃ§Ã£o - Ã© seu blueprint para o sucesso. Cada tÃ©cnica foi escolhida, cada estratÃ©gia foi validada, cada detalhe foi considerado.

O que separa os vencedores dos participantes Ã© uma coisa: **execuÃ§Ã£o implacÃ¡vel de uma estratÃ©gia superior.**

VocÃª tem a estratÃ©gia. VocÃª tem o plano. VocÃª tem as ferramentas.

**Agora vÃ¡ lÃ¡ e conquiste o primeiro lugar! ğŸ†**

---

*"Preparation meets opportunity. Success is not an accident - it's a choice. Your choice starts now."*

**BOA SORTE E QUE A VITÃ“RIA SEJA SUA! ğŸš€ğŸ†**

---

*Documento finalizado. EstratÃ©gia completa. VitÃ³ria ao alcance.*

*VersÃ£o: 2.0 Final | Data: 2025 | Status: Ready to Win*
